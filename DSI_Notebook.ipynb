{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DSI_Notebook.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOkzok81+B6TlONn1Ej9EVB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejlibre/Pre-DSI/blob/dev/DSI_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About this Notebook"
      ],
      "metadata": {
        "id": "RcnQgB3y7LvX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is a training assignment for the DSI program. The aim is to familiarise with basic tools used in data science, such as using pandas for handling dataframes, scikit learn and pytorch for machine learning.\n",
        "\n",
        "This notebook is available in a [github repository](https://github.com/tejlibre/Pre-DSI)"
      ],
      "metadata": {
        "id": "4z7EHgX37YUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Contents\n",
        "\n",
        "These are the tasks to be completed in this assignment\n",
        "\n",
        "0. Access some historical IMDB data files from the shared drive: https://drive.google.com/drive/folders/1dl6nw0HO9XVrT8dSBJHHn3mDW9EWQpXS?usp=sharing\n",
        "\n",
        "1. Read the files 'title.basics.tsv.gz', 'title.akas.tsv.gz' and 'title.ratings.tsv.gz' into three separate dataframes using the read_csv method in Pandas.\n",
        "\n",
        "2. Drop duplicates in all the dataframes, if there are any.\n",
        "\n",
        "3 Using the Pandas 'merge' method, combine all three dataframes using the Title ID (titleID or tconst) to perform the merge and save it into a new dataframe.\n",
        "\n",
        "*  3.1 How many lines does the resulting dataframe have if you use an inner merge or outer merge? Make sure you understand the difference.\n",
        "\n",
        "*  3.2 Using the unique() method, compute how many different 'titleTypes' there are\n",
        "\n",
        "4. Make a new dataframe from step 3 by selecting only rows corresponding to English-language films ('en') OR US-region films ('US') AND only those that are \n",
        "movies (using the 'titleType' column). Put the resulting data into a new dataframe; call it df_new.\n",
        "\n",
        "5. Add a new column to df_new with column title 'log10Votes' which gives the log_10 number of the 'numVotes' column.\n",
        "\n",
        "6. Lower the case of all text in the 'genres' column.\n",
        "\n",
        "7. Using Groupby (or other technique) group all data by 'genres' and display the top 10\n",
        "highest genres by\n",
        "\n",
        "*  7.1 mean number of log10Votes\n",
        " \n",
        "*  7.2 mean averageRating\n",
        "\n",
        "8. Using ‘groupby’ group all data by averageRating and make a scatter plot of averageRating vs log10Votes.\n",
        "\n",
        "9. Perform linear regression on your data (averageRating vs log10Votes) created in the previous step in three different ways:\n",
        "\n",
        "*  9.1 Using sklearn\n",
        "\n",
        "*  9.2 Using scipy\n",
        "\n",
        "*  9.3 Using pytorch\n",
        "\n",
        "Ensure that you get the same result in each case (or explain why the results are different). You will need to install the corresponding packages. If you wanted to build a better regression model what would you do?\n",
        "\n",
        "10. You should commit at least three different versions of your notebook to your github account to demonstrate that you know the basics of using git for version control.\n",
        "\n",
        "11. Share your notebook with us as a Google Colab notebook. NB: MAKE SURE TO MAKE IT PUBLIC. Include your github account in your Colab notebook intro and make sure your commits are public.\n",
        "\n"
      ],
      "metadata": {
        "id": "MOxtRJuv7AwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "2UfTEbJOqD0k"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Access some historical IMDB data files from the shared drive:\n",
        "https://drive.google.com/drive/folders/1dl6nw0HO9XVrT8dSBJHHn3mDW9EWQpXS?usp=sharing"
      ],
      "metadata": {
        "id": "Cmg1HRs08Z-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# wget https://drive.google.com/uc?export=download&id=FILEID\n",
        "#!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=FILEID' -O FILENAME\n",
        "\n",
        "#https://drive.google.com/file/d/1wWbLYAsVldXrnOxdJlu4URsPynYykY22/view?usp=sharing title.ratings.tsv.gz\n",
        "#https://drive.google.com/file/d/1ctjnShHP2qNA1l2nKi1l4qWe3hE5SsPH/view?usp=sharing title.basics.tsv.gz\n",
        "#https://drive.google.com/file/d/1jHKr1FOCigt15gVhrNmtHCHRgzzR1ZGc/view?usp=sharing title.akas.tsv.gz\n",
        "\n",
        "\n",
        "#!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1wWbLYAsVldXrnOxdJlu4URsPynYykY22' -O title.ratings.tsv.gz\n",
        "#!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1ctjnShHP2qNA1l2nKi1l4qWe3hE5SsPH' -O title.basics.tsv.gz\n",
        "#!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1jHKr1FOCigt15gVhrNmtHCHRgzzR1ZGc' -O title.akas.tsv.gz\n",
        "\n",
        "#gdown --folder --id '1dl6nw0HO9XVrT8dSBJHHn3mDW9EWQpXS'\n",
        "# This chunk works.\n",
        "!pip uninstall --yes gdown # After running this line, restart Colab runtime.\n",
        "\n",
        "!pip install gdown -U --no-cache-dir\n",
        "\n",
        "import gdown\n",
        "#url = 'https://drive.google.com/drive/folders/1dl6nw0HO9XVrT8dSBJHHn3mDW9EWQpXS'\n",
        "#gdown.download_folder(url, quiet=True)\n",
        "\n",
        "\n",
        "!gdown --fuzzy 'https://drive.google.com/file/d/1wWbLYAsVldXrnOxdJlu4URsPynYykY22/view?usp=sharing'\n",
        "!gdown --fuzzy 'https://drive.google.com/file/d/1ctjnShHP2qNA1l2nKi1l4qWe3hE5SsPH/view?usp=sharing'\n",
        "!gdown --fuzzy 'https://drive.google.com/file/d/1jHKr1FOCigt15gVhrNmtHCHRgzzR1ZGc/view?usp=sharing'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rDEzAlFuPOT",
        "outputId": "4bd2a131-8832-40ec-ecbe-b722e78e520d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gdown 3.6.4\n",
            "Uninstalling gdown-3.6.4:\n",
            "  Successfully uninstalled gdown-3.6.4\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.2.0.tar.gz (13 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.62.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-4.2.0-py3-none-any.whl size=14262 sha256=f3f7a48b3902a12bc5ad976d06daeea4c7ad8f07449a09b708a4bcf695b3b87c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-uhlv7qzu/wheels/8c/17/ff/58721d1fabdb87c21a0529948cf39e2be9af90ddbe4ad65944\n",
            "Successfully built gdown\n",
            "Installing collected packages: gdown\n",
            "Successfully installed gdown-4.2.0\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wWbLYAsVldXrnOxdJlu4URsPynYykY22\n",
            "To: /content/title.ratings.tsv.gz\n",
            "100% 4.32M/4.32M [00:00<00:00, 76.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ctjnShHP2qNA1l2nKi1l4qWe3hE5SsPH\n",
            "To: /content/title.basics.tsv.gz\n",
            "100% 96.1M/96.1M [00:01<00:00, 66.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1jHKr1FOCigt15gVhrNmtHCHRgzzR1ZGc\n",
            "To: /content/title.akas.tsv.gz\n",
            "100% 54.1M/54.1M [00:00<00:00, 138MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gunzip *.gz\n",
        "!ls -latr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnvHr1FXzrco",
        "outputId": "5416b1c1-fce5-404a-e860-3e51495e973b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 632264\n",
            "drwxr-xr-x 4 root root      4096 Dec 23 14:32 .config\n",
            "drwxr-xr-x 1 root root      4096 Dec 23 14:32 sample_data\n",
            "drwxr-xr-x 1 root root      4096 Jan 10 08:24 ..\n",
            "-rw-r--r-- 1 root root  14866228 Jan 10 08:27 title.ratings.tsv\n",
            "-rw-r--r-- 1 root root 452106547 Jan 10 08:28 title.basics.tsv\n",
            "-rw-r--r-- 1 root root 180436875 Jan 10 08:28 title.akas.tsv\n",
            "drwxr-xr-x 1 root root      4096 Jan 10 08:28 .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Read the files 'title.basics.tsv.gz', 'title.akas.tsv.gz' and 'title.ratings.tsv.gz' into three separate dataframes using the read_csv method in Pandas.\n"
      ],
      "metadata": {
        "id": "iDIFx61t8ixW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ratings = pd.read_csv('./title.ratings.tsv')\n",
        "#basics = pd.read_csv('./title.basics.tsv')\n",
        "#akas = pd.read_csv('./title.akas.tsv')\n",
        "# parse error, need to specify delimiter TAB\n",
        "#ParserError: '\t' expected after '\"' : quotes used in cells so use argument quoting=3\n",
        "ratings = pd.read_csv(\"./title.ratings.tsv\", delimiter=\"\\t\", engine=\"python\", quoting=3)\n",
        "basics = pd.read_csv(\"./title.basics.tsv\", delimiter=\"\\t\", engine=\"python\", quoting=3)\n",
        "akas = pd.read_csv(\"./title.akas.tsv\", delimiter=\"\\t\", engine=\"python\", quoting=3)\n"
      ],
      "metadata": {
        "id": "vh66EtZkzz3y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  2. Drop duplicates in all the dataframes, if there are any."
      ],
      "metadata": {
        "id": "UhMWKqmm8-BE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratings.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Enxk2SMB06sP",
        "outputId": "7cc3825d-dd41-46e7-9fdf-65d055ba98b5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tconst           874203\n",
              "averageRating    874203\n",
              "numVotes         874203\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basics.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C83wvEgb9rfj",
        "outputId": "579d9c58-e6b9-49e5-b661-af7e28504e7c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tconst            5330276\n",
              "titleType         5330276\n",
              "primaryTitle      5330270\n",
              "originalTitle     5330270\n",
              "isAdult           5330276\n",
              "startYear         5330276\n",
              "endYear           5330276\n",
              "runtimeMinutes    5330276\n",
              "genres            5330276\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "akas.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WICuHaJT9vhj",
        "outputId": "39ab9682-2918-4756-d475-5142edd4f40d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "titleId            3674735\n",
              "ordering           3674735\n",
              "title              3674735\n",
              "region             3674678\n",
              "language           3674735\n",
              "types              3674735\n",
              "attributes         3674735\n",
              "isOriginalTitle    3674735\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = ratings.drop_duplicates()\n",
        "basics = basics.drop_duplicates()\n",
        "akas = akas.drop_duplicates()"
      ],
      "metadata": {
        "id": "uhlxff6a92vZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sP4VDtFG-DYY",
        "outputId": "692937de-b324-43f1-f992-61ba9ad82edf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tconst           874203\n",
              "averageRating    874203\n",
              "numVotes         874203\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basics.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JSBo-Ho-F-S",
        "outputId": "0ed50186-4022-45d7-8269-7de62b7fb53b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tconst            5330276\n",
              "titleType         5330276\n",
              "primaryTitle      5330270\n",
              "originalTitle     5330270\n",
              "isAdult           5330276\n",
              "startYear         5330276\n",
              "endYear           5330276\n",
              "runtimeMinutes    5330276\n",
              "genres            5330276\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "akas.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vh9_7_qa-H1I",
        "outputId": "1e66b9b2-1bad-41d7-a773-d4020a5aeed2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "titleId            3674735\n",
              "ordering           3674735\n",
              "title              3674735\n",
              "region             3674678\n",
              "language           3674735\n",
              "types              3674735\n",
              "attributes         3674735\n",
              "isOriginalTitle    3674735\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.0 Using the Pandas 'merge' method, combine all three dataframes using the Title ID (titleID or tconst) to perform the merge and save it into a new dataframe.\n",
        "\n",
        "3.1 How many lines does the resulting dataframe have if you use an inner merge or outer merge? Make sure you understand the difference.\n",
        "\n",
        "3.2 Using the unique() method, compute how many different 'titleTypes' there are\n"
      ],
      "metadata": {
        "id": "LDKdNXz6-Qhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merge_inner = ratings\n",
        "merge_inner = merge_inner.merge(basics, how='inner', left_on='tconst', right_on='tconst')\n",
        "merge_inner = merge_inner.merge(akas, how='inner', left_on='tconst', right_on='titleId')\n",
        "merge_inner.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "384eQVp6BK06",
        "outputId": "4b8c8be2-6f6e-4b33-d4fb-55a32e285762"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tconst             1885165\n",
              "averageRating      1885165\n",
              "numVotes           1885165\n",
              "titleType          1885165\n",
              "primaryTitle       1885165\n",
              "originalTitle      1885165\n",
              "isAdult            1885165\n",
              "startYear          1885165\n",
              "endYear            1885165\n",
              "runtimeMinutes     1885165\n",
              "genres             1885165\n",
              "titleId            1885165\n",
              "ordering           1885165\n",
              "title              1885165\n",
              "region             1885157\n",
              "language           1885165\n",
              "types              1885165\n",
              "attributes         1885165\n",
              "isOriginalTitle    1885165\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merge_outer = ratings\n",
        "merge_outer = merge_outer.merge(basics, how='outer', left_on='tconst', right_on='tconst')\n",
        "merge_outer = merge_outer.merge(akas, how='outer', left_on='tconst', right_on='titleId')\n",
        "merge_outer.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hx1WdItAA88c",
        "outputId": "ad8b993a-9082-439d-f4d8-5fda81083f71"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tconst             6910572\n",
              "averageRating      2133310\n",
              "numVotes           2133310\n",
              "titleType          6910572\n",
              "primaryTitle       6910566\n",
              "originalTitle      6910566\n",
              "isAdult            6910572\n",
              "startYear          6910572\n",
              "endYear            6910572\n",
              "runtimeMinutes     6910572\n",
              "genres             6910572\n",
              "titleId            3674735\n",
              "ordering           3674735\n",
              "title              3674735\n",
              "region             3674678\n",
              "language           3674735\n",
              "types              3674735\n",
              "attributes         3674735\n",
              "isOriginalTitle    3674735\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merge_inner.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "NIUHcrDqFUKV",
        "outputId": "79359275-2b13-4ea4-c84b-ab23fdfa7012"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0726729a-5ff8-4483-976d-5637871aa12b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tconst</th>\n",
              "      <th>averageRating</th>\n",
              "      <th>numVotes</th>\n",
              "      <th>titleType</th>\n",
              "      <th>primaryTitle</th>\n",
              "      <th>originalTitle</th>\n",
              "      <th>isAdult</th>\n",
              "      <th>startYear</th>\n",
              "      <th>endYear</th>\n",
              "      <th>runtimeMinutes</th>\n",
              "      <th>genres</th>\n",
              "      <th>titleId</th>\n",
              "      <th>ordering</th>\n",
              "      <th>title</th>\n",
              "      <th>region</th>\n",
              "      <th>language</th>\n",
              "      <th>types</th>\n",
              "      <th>attributes</th>\n",
              "      <th>isOriginalTitle</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tt0000001</td>\n",
              "      <td>5.8</td>\n",
              "      <td>1422</td>\n",
              "      <td>short</td>\n",
              "      <td>Carmencita</td>\n",
              "      <td>Carmencita</td>\n",
              "      <td>0</td>\n",
              "      <td>1894</td>\n",
              "      <td>\\N</td>\n",
              "      <td>1</td>\n",
              "      <td>Documentary,Short</td>\n",
              "      <td>tt0000001</td>\n",
              "      <td>1</td>\n",
              "      <td>Carmencita - spanyol tánc</td>\n",
              "      <td>HU</td>\n",
              "      <td>\\N</td>\n",
              "      <td>imdbDisplay</td>\n",
              "      <td>\\N</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tt0000001</td>\n",
              "      <td>5.8</td>\n",
              "      <td>1422</td>\n",
              "      <td>short</td>\n",
              "      <td>Carmencita</td>\n",
              "      <td>Carmencita</td>\n",
              "      <td>0</td>\n",
              "      <td>1894</td>\n",
              "      <td>\\N</td>\n",
              "      <td>1</td>\n",
              "      <td>Documentary,Short</td>\n",
              "      <td>tt0000001</td>\n",
              "      <td>2</td>\n",
              "      <td>Карменсита</td>\n",
              "      <td>RU</td>\n",
              "      <td>\\N</td>\n",
              "      <td>\\N</td>\n",
              "      <td>\\N</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tt0000001</td>\n",
              "      <td>5.8</td>\n",
              "      <td>1422</td>\n",
              "      <td>short</td>\n",
              "      <td>Carmencita</td>\n",
              "      <td>Carmencita</td>\n",
              "      <td>0</td>\n",
              "      <td>1894</td>\n",
              "      <td>\\N</td>\n",
              "      <td>1</td>\n",
              "      <td>Documentary,Short</td>\n",
              "      <td>tt0000001</td>\n",
              "      <td>3</td>\n",
              "      <td>Carmencita</td>\n",
              "      <td>US</td>\n",
              "      <td>\\N</td>\n",
              "      <td>\\N</td>\n",
              "      <td>\\N</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tt0000001</td>\n",
              "      <td>5.8</td>\n",
              "      <td>1422</td>\n",
              "      <td>short</td>\n",
              "      <td>Carmencita</td>\n",
              "      <td>Carmencita</td>\n",
              "      <td>0</td>\n",
              "      <td>1894</td>\n",
              "      <td>\\N</td>\n",
              "      <td>1</td>\n",
              "      <td>Documentary,Short</td>\n",
              "      <td>tt0000001</td>\n",
              "      <td>4</td>\n",
              "      <td>Carmencita</td>\n",
              "      <td>\\N</td>\n",
              "      <td>\\N</td>\n",
              "      <td>original</td>\n",
              "      <td>\\N</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tt0000002</td>\n",
              "      <td>6.4</td>\n",
              "      <td>168</td>\n",
              "      <td>short</td>\n",
              "      <td>Le clown et ses chiens</td>\n",
              "      <td>Le clown et ses chiens</td>\n",
              "      <td>0</td>\n",
              "      <td>1892</td>\n",
              "      <td>\\N</td>\n",
              "      <td>5</td>\n",
              "      <td>Animation,Short</td>\n",
              "      <td>tt0000002</td>\n",
              "      <td>1</td>\n",
              "      <td>Le clown et ses chiens</td>\n",
              "      <td>\\N</td>\n",
              "      <td>\\N</td>\n",
              "      <td>original</td>\n",
              "      <td>\\N</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0726729a-5ff8-4483-976d-5637871aa12b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0726729a-5ff8-4483-976d-5637871aa12b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0726729a-5ff8-4483-976d-5637871aa12b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      tconst  averageRating  numVotes  ...        types attributes isOriginalTitle\n",
              "0  tt0000001            5.8      1422  ...  imdbDisplay         \\N               0\n",
              "1  tt0000001            5.8      1422  ...           \\N         \\N               0\n",
              "2  tt0000001            5.8      1422  ...           \\N         \\N               0\n",
              "3  tt0000001            5.8      1422  ...     original         \\N               1\n",
              "4  tt0000002            6.4       168  ...     original         \\N               1\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merge_outer.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "Eo2vSo3uFb6v",
        "outputId": "70963f1a-c6ac-4dfa-8c7e-51c258111739"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-deaee790-ddd4-4d6d-b6f2-b564e8f5409a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tconst</th>\n",
              "      <th>averageRating</th>\n",
              "      <th>numVotes</th>\n",
              "      <th>titleType</th>\n",
              "      <th>primaryTitle</th>\n",
              "      <th>originalTitle</th>\n",
              "      <th>isAdult</th>\n",
              "      <th>startYear</th>\n",
              "      <th>endYear</th>\n",
              "      <th>runtimeMinutes</th>\n",
              "      <th>genres</th>\n",
              "      <th>titleId</th>\n",
              "      <th>ordering</th>\n",
              "      <th>title</th>\n",
              "      <th>region</th>\n",
              "      <th>language</th>\n",
              "      <th>types</th>\n",
              "      <th>attributes</th>\n",
              "      <th>isOriginalTitle</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tt0000001</td>\n",
              "      <td>5.8</td>\n",
              "      <td>1422.0</td>\n",
              "      <td>short</td>\n",
              "      <td>Carmencita</td>\n",
              "      <td>Carmencita</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1894</td>\n",
              "      <td>\\N</td>\n",
              "      <td>1</td>\n",
              "      <td>Documentary,Short</td>\n",
              "      <td>tt0000001</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Carmencita - spanyol tánc</td>\n",
              "      <td>HU</td>\n",
              "      <td>\\N</td>\n",
              "      <td>imdbDisplay</td>\n",
              "      <td>\\N</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tt0000001</td>\n",
              "      <td>5.8</td>\n",
              "      <td>1422.0</td>\n",
              "      <td>short</td>\n",
              "      <td>Carmencita</td>\n",
              "      <td>Carmencita</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1894</td>\n",
              "      <td>\\N</td>\n",
              "      <td>1</td>\n",
              "      <td>Documentary,Short</td>\n",
              "      <td>tt0000001</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Карменсита</td>\n",
              "      <td>RU</td>\n",
              "      <td>\\N</td>\n",
              "      <td>\\N</td>\n",
              "      <td>\\N</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tt0000001</td>\n",
              "      <td>5.8</td>\n",
              "      <td>1422.0</td>\n",
              "      <td>short</td>\n",
              "      <td>Carmencita</td>\n",
              "      <td>Carmencita</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1894</td>\n",
              "      <td>\\N</td>\n",
              "      <td>1</td>\n",
              "      <td>Documentary,Short</td>\n",
              "      <td>tt0000001</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Carmencita</td>\n",
              "      <td>US</td>\n",
              "      <td>\\N</td>\n",
              "      <td>\\N</td>\n",
              "      <td>\\N</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tt0000001</td>\n",
              "      <td>5.8</td>\n",
              "      <td>1422.0</td>\n",
              "      <td>short</td>\n",
              "      <td>Carmencita</td>\n",
              "      <td>Carmencita</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1894</td>\n",
              "      <td>\\N</td>\n",
              "      <td>1</td>\n",
              "      <td>Documentary,Short</td>\n",
              "      <td>tt0000001</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Carmencita</td>\n",
              "      <td>\\N</td>\n",
              "      <td>\\N</td>\n",
              "      <td>original</td>\n",
              "      <td>\\N</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tt0000002</td>\n",
              "      <td>6.4</td>\n",
              "      <td>168.0</td>\n",
              "      <td>short</td>\n",
              "      <td>Le clown et ses chiens</td>\n",
              "      <td>Le clown et ses chiens</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1892</td>\n",
              "      <td>\\N</td>\n",
              "      <td>5</td>\n",
              "      <td>Animation,Short</td>\n",
              "      <td>tt0000002</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Le clown et ses chiens</td>\n",
              "      <td>\\N</td>\n",
              "      <td>\\N</td>\n",
              "      <td>original</td>\n",
              "      <td>\\N</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-deaee790-ddd4-4d6d-b6f2-b564e8f5409a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-deaee790-ddd4-4d6d-b6f2-b564e8f5409a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-deaee790-ddd4-4d6d-b6f2-b564e8f5409a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      tconst  averageRating  numVotes  ...        types attributes isOriginalTitle\n",
              "0  tt0000001            5.8    1422.0  ...  imdbDisplay         \\N               0\n",
              "1  tt0000001            5.8    1422.0  ...           \\N         \\N               0\n",
              "2  tt0000001            5.8    1422.0  ...           \\N         \\N               0\n",
              "3  tt0000001            5.8    1422.0  ...     original         \\N               1\n",
              "4  tt0000002            6.4     168.0  ...     original         \\N               1\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titleTypes = merge_inner.titleType.unique()\n",
        "print(titleTypes)\n",
        "print(titleTypes.size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAaj0O_NJ3ns",
        "outputId": "396346ba-6211-4c68-81ef-63bd5e7e0a0e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['short' 'movie' 'tvMovie' 'tvSeries' 'tvEpisode' 'tvShort' 'tvMiniSeries'\n",
            " 'tvSpecial' 'video' 'videoGame']\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titleTypes = merge_outer.titleType.unique()\n",
        "print(titleTypes)\n",
        "print(titleTypes.size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wetn_xidJ9I5",
        "outputId": "bf408b22-be25-4be1-fd1b-b99e28d83e1e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['short' 'movie' 'tvMovie' 'tvSeries' 'tvEpisode' 'tvShort' 'tvMiniSeries'\n",
            " 'tvSpecial' 'video' 'videoGame' nan]\n",
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Make a new dataframe from step 3 by selecting only rows corresponding to English-language films ('en') OR US-region films ('US') AND only those that are \n",
        "movies (using the 'titleType' column). Put the resulting data into a new dataframe; call it df_new."
      ],
      "metadata": {
        "id": "JkONO8BuZCUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merge_inner.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7COylJlqWdQ",
        "outputId": "1978d735-2bc5-46b5-a344-b699ed9a53b9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['tconst', 'averageRating', 'numVotes', 'titleType', 'primaryTitle',\n",
              "       'originalTitle', 'isAdult', 'startYear', 'endYear', 'runtimeMinutes',\n",
              "       'genres', 'titleId', 'ordering', 'title', 'region', 'language', 'types',\n",
              "       'attributes', 'isOriginalTitle'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merge_inner.titleType.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlDMjRIfqaPR",
        "outputId": "8204e59a-b3cb-4a63-dddd-f27602aae9fd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['short', 'movie', 'tvMovie', 'tvSeries', 'tvEpisode', 'tvShort',\n",
              "       'tvMiniSeries', 'tvSpecial', 'video', 'videoGame'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merge_inner.language.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keopbVzUqeQZ",
        "outputId": "18f8914a-3eed-439c-a58f-ee2530e198c1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['\\\\N', 'sv', 'en', 'tr', 'es', 'sr', 'cs', 'fa', 'fr', 'bg', 'ca',\n",
              "       'nl', 'qbn', 'pt', 'ru', 'qbp', 'ar', 'rn', 'de', 'yi', 'uk', 'ka',\n",
              "       'he', 'hr', 'sl', 'tg', 'sk', 'cmn', 'kk', 'da', 'el', 'fi', 'it',\n",
              "       'gsw', 'pl', 'mr', 'qbo', 'gl', 'ms', 'th', 'ta', 'af', 'la', 'hy',\n",
              "       'hi', 'ur', 'yue', 'te', 'bn', 'lt', 'mk', 'et', 'gd', 'tl', 'lv',\n",
              "       'bs', 'cy', 'id', 'qal', 'goh', 'eu', 'ml', 'ro', 'hu', 'pa', 'uz',\n",
              "       'ja', 'wo', 'no', 'is', 'sq', 'vi', 'ga', 'gu', 'nqo', 'kn', 'xh',\n",
              "       'mi', 'ps', 'az', 'ky', 'fro', 'myv', 'ko', 'iu', 'st', 'zu', 'tn',\n",
              "       'zh', 'ku'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merge_inner.region.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoG616TNqguC",
        "outputId": "2dd0acb2-40cc-4f5d-ca84-7b7328ea83ce"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['HU', 'RU', 'US', '\\\\N', 'FR', 'RO', 'GB', 'PT', 'ES', 'UY', 'DE',\n",
              "       'IT', 'FI', 'PL', 'AR', 'BR', 'XWW', 'TR', 'DK', 'XEU', 'SK', 'CZ',\n",
              "       'SE', 'MX', 'GR', 'RS', 'XYU', 'AT', 'VE', 'CSHH', 'JP', 'AU',\n",
              "       'NL', 'SI', 'NO', 'IR', 'UA', 'CA', 'CO', 'BG', 'BE', 'IN', 'DZ',\n",
              "       'BF', 'XWG', 'NZ', 'VN', 'SUHH', 'EE', 'IS', 'DDDE', 'HR', 'CL',\n",
              "       'LT', 'EG', 'GE', 'CH', 'PA', 'HK', 'CN', 'XSI', 'IE', 'XSA', 'PE',\n",
              "       'IL', 'CU', 'KR', 'BA', 'BUMM', 'YUCS', 'XPI', 'BJ', 'PR', 'MY',\n",
              "       'CM', 'AZ', 'ZA', 'TH', 'BO', 'DO', 'AL', 'EC', 'SG', 'LV', 'MA',\n",
              "       'LI', 'LU', 'ID', 'PH', 'MZ', 'BM', 'PY', 'TW', 'JM', 'MD', 'LB',\n",
              "       'TM', 'GL', 'MK', 'CR', 'TN', 'JO', 'KG', 'PK', 'LK', 'GT', 'XAS',\n",
              "       'SN', 'TJ', 'NE', 'CI', 'MC', 'GH', 'TT', 'BS', 'SY', 'AO', 'KH',\n",
              "       'GA', 'SV', 'MR', 'CY', 'ET', 'ML', 'NG', 'UZ', 'LY', 'SR', 'AM',\n",
              "       'PG', 'IQ', 'BW', 'NI', 'ZM', 'CG', 'KZ', 'KP', 'XKO', 'GI', 'LA',\n",
              "       'GW', 'BD', 'ZW', 'NP', 'BY', 'MN', 'FO', 'PS', 'MO', 'VDVN', 'GN',\n",
              "       'AW', 'KW', 'HT', 'TZ', 'SL', 'SM', 'AF', 'TO', 'BT', 'MT', 'TG',\n",
              "       'HN', 'AD', 'ZRCD', 'GY', 'CSXX', 'ER', nan, 'SA', 'SO', 'IM',\n",
              "       'MU', 'BH', 'KE', 'GP', 'RW', 'TD', 'XKV', 'CV', 'AE', 'SD', 'AG',\n",
              "       'NU', 'OM', 'CD', 'QA', 'BB', 'YE', 'LS', 'LC', 'ME', 'MV', 'BI',\n",
              "       'MH', 'VI', 'DM', 'XNA', 'BZ', 'FJ', 'MW', 'LR', 'SZ', 'UG', 'MG',\n",
              "       'AN', 'VU', 'GU', 'MM', 'TV', 'WS', 'NC', 'BN', 'VA', 'GQ', 'TL',\n",
              "       'MQ', 'GM', 'PW', 'PF', 'GD', 'CF', 'RE', 'SB'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merge_inner[((merge_inner[\"language\"]==\"en\") | (merge_inner[\"region\"]==\"US\")) & (merge_inner[\"titleType\"]==\"movie\")].loc[:,[\"primaryTitle\",\"region\",\"language\"]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "wspNCcxWqtxL",
        "outputId": "7dade5ba-78dc-4520-b78e-ca87856bb871"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-841e11fd-c366-45d0-870d-3a2c622fad4f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>primaryTitle</th>\n",
              "      <th>region</th>\n",
              "      <th>language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>Miss Jerry</td>\n",
              "      <td>US</td>\n",
              "      <td>\\N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>574</th>\n",
              "      <td>The Corbett-Fitzsimmons Fight</td>\n",
              "      <td>US</td>\n",
              "      <td>\\N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1446</th>\n",
              "      <td>Hamlet</td>\n",
              "      <td>US</td>\n",
              "      <td>\\N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1520</th>\n",
              "      <td>The Fairylogue and Radio-Plays</td>\n",
              "      <td>US</td>\n",
              "      <td>\\N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1797</th>\n",
              "      <td>Hamlet, Prince of Denmark</td>\n",
              "      <td>US</td>\n",
              "      <td>\\N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1885040</th>\n",
              "      <td>Temporary Difficulties</td>\n",
              "      <td>US</td>\n",
              "      <td>\\N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1885103</th>\n",
              "      <td>Bearer</td>\n",
              "      <td>XWW</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1885112</th>\n",
              "      <td>Tuulte tahutud maa</td>\n",
              "      <td>XWW</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1885138</th>\n",
              "      <td>Aickarakkonathe Bhishaguaranmaar</td>\n",
              "      <td>IN</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1885140</th>\n",
              "      <td>What Have We Done to Deserve This?</td>\n",
              "      <td>XWW</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>179685 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-841e11fd-c366-45d0-870d-3a2c622fad4f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-841e11fd-c366-45d0-870d-3a2c622fad4f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-841e11fd-c366-45d0-870d-3a2c622fad4f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                               primaryTitle region language\n",
              "51                               Miss Jerry     US       \\N\n",
              "574           The Corbett-Fitzsimmons Fight     US       \\N\n",
              "1446                                 Hamlet     US       \\N\n",
              "1520         The Fairylogue and Radio-Plays     US       \\N\n",
              "1797              Hamlet, Prince of Denmark     US       \\N\n",
              "...                                     ...    ...      ...\n",
              "1885040              Temporary Difficulties     US       \\N\n",
              "1885103                              Bearer    XWW       en\n",
              "1885112                  Tuulte tahutud maa    XWW       en\n",
              "1885138    Aickarakkonathe Bhishaguaranmaar     IN       en\n",
              "1885140  What Have We Done to Deserve This?    XWW       en\n",
              "\n",
              "[179685 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_new = merge_inner[((merge_inner[\"language\"]==\"en\") | (merge_inner[\"region\"]==\"US\")) & (merge_inner[\"titleType\"]==\"movie\")]"
      ],
      "metadata": {
        "id": "5CmUA9taZLb8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Add a new column to df_new with column title 'log10Votes' which gives the log_10 number of the 'numVotes' column.\n"
      ],
      "metadata": {
        "id": "k7ncRaierFS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_new[\"numVotes\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwhH8PXNrAXr",
        "outputId": "b4246504-3ab1-4acf-d88f-b7113f292bbc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51          70\n",
              "574        266\n",
              "1446        10\n",
              "1520        26\n",
              "1797        16\n",
              "          ... \n",
              "1885040     20\n",
              "1885103      9\n",
              "1885112     17\n",
              "1885138    213\n",
              "1885140      8\n",
              "Name: numVotes, Length: 179685, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.loc[:,\"log10Votes\"]=np.log10(df_new[\"numVotes\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dzd3TrRCrSi-",
        "outputId": "2511c3ce-f3bf-463a-ac08-cb15efc7e2aa"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1596: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.obj[key] = _infer_fill_value(value)\n",
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(ilocs[0], value)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_new[\"log10Votes\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0INo2tOu0g-",
        "outputId": "db4e04c5-3689-452f-fdc1-b37c2f7e7ca0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51         1.845098\n",
              "574        2.424882\n",
              "1446       1.000000\n",
              "1520       1.414973\n",
              "1797       1.204120\n",
              "             ...   \n",
              "1885040    1.301030\n",
              "1885103    0.954243\n",
              "1885112    1.230449\n",
              "1885138    2.328380\n",
              "1885140    0.903090\n",
              "Name: log10Votes, Length: 179685, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  7. Lower the case of all text in the 'genres' column.\n"
      ],
      "metadata": {
        "id": "DW59C_zSu40H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_new[\"genres\"] = df_new[\"genres\"].apply(lambda x: x.lower())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj3ZB3Q4y81n",
        "outputId": "89a4931d-47dc-4985-fc48-3bbaf94b944b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Using Groupby (or other technique) group all data by 'genres' and display the top 10\n",
        "highest genres by\n",
        "\n",
        " 7.1 mean number of log10Votes\n",
        "\n",
        " 7.2 mean averageRating"
      ],
      "metadata": {
        "id": "SqDyCKBGzFVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_rating = df_new.groupby(\"genres\")[\"averageRating\"].mean()\n",
        "mean_rating.sort_values(ascending=False).head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Edt5tLUQzNQZ",
        "outputId": "7c519770-00b8-4dce-ca79-9a72aa9f9b27"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "genres\n",
              "documentary,history,western        9.300000\n",
              "history,sport                      9.200000\n",
              "documentary,news,reality-tv        8.800000\n",
              "animation,crime,documentary        8.525000\n",
              "biography,history,music            8.500000\n",
              "adventure,documentary,western      8.433333\n",
              "documentary,drama,thriller         8.433333\n",
              "comedy,mystery,sport               8.400000\n",
              "biography,documentary,talk-show    8.300000\n",
              "action,documentary,fantasy         8.300000\n",
              "Name: averageRating, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_log10Votes = df_new.groupby(\"genres\")[\"log10Votes\"].mean()\n",
        "mean_log10Votes.sort_values(ascending=False).head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA5Snrl_73CR",
        "outputId": "b2391d58-1153-4153-8cf3-9a40daf3600f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "genres\n",
              "action,fantasy,war            5.405722\n",
              "action,family,romance         4.801753\n",
              "animation,drama,war           4.396304\n",
              "family,music,musical          4.396252\n",
              "documentary,sport,thriller    4.386731\n",
              "action,adventure,sci-fi       4.384822\n",
              "horror,musical,sci-fi         4.340484\n",
              "biography,fantasy,horror      4.271842\n",
              "action,adventure,thriller     4.142012\n",
              "adventure,drama,sci-fi        4.123983\n",
              "Name: log10Votes, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Using ‘groupby’ group all data by averageRating and make a scatter plot of averageRating vs log10Votes."
      ],
      "metadata": {
        "id": "jLXAGE0l-a_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = df_new.groupby(\"averageRating\")[\"log10Votes\"].mean()"
      ],
      "metadata": {
        "id": "il6zfLUo-aMH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "NuYIqewf-jmj"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = plt.scatter(ratings.index, ratings.values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "50MuHX10C2N_",
        "outputId": "6a1db3a3-c936-4939-8e71-7e82ff5c14a6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratings.reset_index().plot(kind='scatter', x='averageRating', y='log10Votes')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "HAeWsV4LC5_T",
        "outputId": "826a031a-5319-484f-8423-e3875b85c203"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f42edc68950>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1Gk9a8VAGZmV"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  9. Perform linear regression on your data (averageRating vs log10Votes) created in the previous step in three different ways:\n",
        "\n",
        "*  9.1 Using sklearn\n",
        "\n",
        "*  9.2 Using scipy\n",
        "\n",
        "*  9.3 Using pytorch\n",
        "\n",
        "Ensure that you get the same result in each case (or explain why the results are different). You will need to install the corresponding packages. If you wanted to build a better regression model what would you do?\n"
      ],
      "metadata": {
        "id": "xZnx0jTKGaMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n"
      ],
      "metadata": {
        "id": "_ohGzJoqGeUo"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extract x and y arrays\n",
        "data = ratings.reset_index()\n",
        "\n",
        "train = data.sample(frac=0.8, random_state=25)\n",
        "test = data.drop(train.index)\n",
        "\n",
        "x_train = train[\"averageRating\"].to_numpy().reshape(-1,1)\n",
        "y_train = train[\"log10Votes\"].to_numpy().reshape(-1,1)\n",
        "\n",
        "x_test = test[\"averageRating\"].to_numpy().reshape(-1,1)\n",
        "y_test = test[\"log10Votes\"].to_numpy().reshape(-1,1)"
      ],
      "metadata": {
        "id": "sCzxmlvGOxjZ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# linear regression using scikitlearn\n",
        "\n",
        "%time reg = LinearRegression().fit(x_train, y_train)\n",
        "print(\"Coefficients: \\n\", reg.coef_)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = reg.predict(x_test)\n",
        "\n",
        "# The mean squared error\n",
        "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n",
        "# The coefficient of determination: 1 is perfect prediction\n",
        "print(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv_EqKVVPdeN",
        "outputId": "f757fb7f-b0d2-4d8b-914f-e62be98b4757"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.23 ms, sys: 10 µs, total: 3.24 ms\n",
            "Wall time: 21.6 ms\n",
            "Coefficients: \n",
            " [[-0.0828916]]\n",
            "Mean squared error: 0.15\n",
            "Coefficient of determination: -0.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# linear regression using scipy\n",
        "x_train2 = np.concatenate( x_train, axis=0 )\n",
        "y_train2 = np.concatenate( y_train, axis=0 )\n",
        "\n",
        "%time reg2 = stats.linregress(x_train2, y_train2)\n",
        "print(\"Coefficients: \\n\", reg2.slope)\n",
        "\n",
        "# Evaluate\n",
        "y_pred2 = reg2.intercept + reg2.slope * x_test\n",
        "\n",
        "# The mean squared error\n",
        "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred2))\n",
        "# The coefficient of determination: 1 is perfect prediction\n",
        "print(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOmjUfc7VE98",
        "outputId": "7ea6fcb6-6007-4ce7-8e79-198585e56e94"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 990 µs, sys: 0 ns, total: 990 µs\n",
            "Wall time: 1.21 ms\n",
            "Coefficients: \n",
            " -0.08289160469409297\n",
            "Mean squared error: 0.15\n",
            "Coefficient of determination: -0.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# linear regression using pytorch\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "class linearRegression(torch.nn.Module):\n",
        "    def __init__(self, inputSize, outputSize):\n",
        "        super(linearRegression, self).__init__()\n",
        "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "inputDim = 1        # takes variable 'x' \n",
        "outputDim = 1       # takes variable 'y'\n",
        "learningRate = 0.01 \n",
        "epochs = 1000\n",
        "\n",
        "model = linearRegression(inputDim, outputDim)\n",
        "\n",
        "criterion = torch.nn.MSELoss() \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n"
      ],
      "metadata": {
        "id": "ctPeyN3SZUBy"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_values_train = np.array(x_train, dtype=np.float32)\n",
        "y_values_train = np.array(y_train, dtype=np.float32)"
      ],
      "metadata": {
        "id": "TTzEaJGLB4X6"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset parameters before training\n",
        "for layer in model.children():\n",
        "    if hasattr(layer, 'reset_parameters'):\n",
        "        layer.reset_parameters()\n",
        "\n",
        "# Training neural network\n",
        "for epoch in range(epochs):\n",
        "    # Converting inputs and labels to Variable\n",
        "    inputs = Variable(torch.from_numpy(x_values_train))\n",
        "    labels = Variable(torch.from_numpy(y_values_train))\n",
        "\n",
        "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # get output from the model, given the inputs\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # get loss for the predicted output\n",
        "    loss = criterion(outputs, labels)\n",
        "    print(loss)\n",
        "    # get gradients w.r.t to parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tLs4qykB5MF",
        "outputId": "8819c00a-c1f0-4d06-95c2-88b076325150"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(23.7377, grad_fn=<MseLossBackward0>)\n",
            "epoch 0, loss 23.737707138061523\n",
            "tensor(2.4680, grad_fn=<MseLossBackward0>)\n",
            "epoch 1, loss 2.468022346496582\n",
            "tensor(1.5228, grad_fn=<MseLossBackward0>)\n",
            "epoch 2, loss 1.5228382349014282\n",
            "tensor(1.4722, grad_fn=<MseLossBackward0>)\n",
            "epoch 3, loss 1.4722057580947876\n",
            "tensor(1.4610, grad_fn=<MseLossBackward0>)\n",
            "epoch 4, loss 1.4610050916671753\n",
            "tensor(1.4516, grad_fn=<MseLossBackward0>)\n",
            "epoch 5, loss 1.4516016244888306\n",
            "tensor(1.4423, grad_fn=<MseLossBackward0>)\n",
            "epoch 6, loss 1.4423385858535767\n",
            "tensor(1.4331, grad_fn=<MseLossBackward0>)\n",
            "epoch 7, loss 1.4331426620483398\n",
            "tensor(1.4240, grad_fn=<MseLossBackward0>)\n",
            "epoch 8, loss 1.424010157585144\n",
            "tensor(1.4149, grad_fn=<MseLossBackward0>)\n",
            "epoch 9, loss 1.4149409532546997\n",
            "tensor(1.4059, grad_fn=<MseLossBackward0>)\n",
            "epoch 10, loss 1.4059337377548218\n",
            "tensor(1.3970, grad_fn=<MseLossBackward0>)\n",
            "epoch 11, loss 1.3969889879226685\n",
            "tensor(1.3881, grad_fn=<MseLossBackward0>)\n",
            "epoch 12, loss 1.3881051540374756\n",
            "tensor(1.3793, grad_fn=<MseLossBackward0>)\n",
            "epoch 13, loss 1.37928307056427\n",
            "tensor(1.3705, grad_fn=<MseLossBackward0>)\n",
            "epoch 14, loss 1.3705216646194458\n",
            "tensor(1.3618, grad_fn=<MseLossBackward0>)\n",
            "epoch 15, loss 1.3618205785751343\n",
            "tensor(1.3532, grad_fn=<MseLossBackward0>)\n",
            "epoch 16, loss 1.3531794548034668\n",
            "tensor(1.3446, grad_fn=<MseLossBackward0>)\n",
            "epoch 17, loss 1.3445978164672852\n",
            "tensor(1.3361, grad_fn=<MseLossBackward0>)\n",
            "epoch 18, loss 1.3360751867294312\n",
            "tensor(1.3276, grad_fn=<MseLossBackward0>)\n",
            "epoch 19, loss 1.3276113271713257\n",
            "tensor(1.3192, grad_fn=<MseLossBackward0>)\n",
            "epoch 20, loss 1.3192057609558105\n",
            "tensor(1.3109, grad_fn=<MseLossBackward0>)\n",
            "epoch 21, loss 1.3108582496643066\n",
            "tensor(1.3026, grad_fn=<MseLossBackward0>)\n",
            "epoch 22, loss 1.302567958831787\n",
            "tensor(1.2943, grad_fn=<MseLossBackward0>)\n",
            "epoch 23, loss 1.294334888458252\n",
            "tensor(1.2862, grad_fn=<MseLossBackward0>)\n",
            "epoch 24, loss 1.286158561706543\n",
            "tensor(1.2780, grad_fn=<MseLossBackward0>)\n",
            "epoch 25, loss 1.278038501739502\n",
            "tensor(1.2700, grad_fn=<MseLossBackward0>)\n",
            "epoch 26, loss 1.2699743509292603\n",
            "tensor(1.2620, grad_fn=<MseLossBackward0>)\n",
            "epoch 27, loss 1.2619657516479492\n",
            "tensor(1.2540, grad_fn=<MseLossBackward0>)\n",
            "epoch 28, loss 1.2540122270584106\n",
            "tensor(1.2461, grad_fn=<MseLossBackward0>)\n",
            "epoch 29, loss 1.2461135387420654\n",
            "tensor(1.2383, grad_fn=<MseLossBackward0>)\n",
            "epoch 30, loss 1.238269329071045\n",
            "tensor(1.2305, grad_fn=<MseLossBackward0>)\n",
            "epoch 31, loss 1.2304790019989014\n",
            "tensor(1.2227, grad_fn=<MseLossBackward0>)\n",
            "epoch 32, loss 1.2227426767349243\n",
            "tensor(1.2151, grad_fn=<MseLossBackward0>)\n",
            "epoch 33, loss 1.2150592803955078\n",
            "tensor(1.2074, grad_fn=<MseLossBackward0>)\n",
            "epoch 34, loss 1.2074289321899414\n",
            "tensor(1.1999, grad_fn=<MseLossBackward0>)\n",
            "epoch 35, loss 1.1998509168624878\n",
            "tensor(1.1923, grad_fn=<MseLossBackward0>)\n",
            "epoch 36, loss 1.1923255920410156\n",
            "tensor(1.1849, grad_fn=<MseLossBackward0>)\n",
            "epoch 37, loss 1.1848514080047607\n",
            "tensor(1.1774, grad_fn=<MseLossBackward0>)\n",
            "epoch 38, loss 1.17742919921875\n",
            "tensor(1.1701, grad_fn=<MseLossBackward0>)\n",
            "epoch 39, loss 1.170058012008667\n",
            "tensor(1.1627, grad_fn=<MseLossBackward0>)\n",
            "epoch 40, loss 1.1627377271652222\n",
            "tensor(1.1555, grad_fn=<MseLossBackward0>)\n",
            "epoch 41, loss 1.1554675102233887\n",
            "tensor(1.1482, grad_fn=<MseLossBackward0>)\n",
            "epoch 42, loss 1.1482475996017456\n",
            "tensor(1.1411, grad_fn=<MseLossBackward0>)\n",
            "epoch 43, loss 1.1410772800445557\n",
            "tensor(1.1340, grad_fn=<MseLossBackward0>)\n",
            "epoch 44, loss 1.1339566707611084\n",
            "tensor(1.1269, grad_fn=<MseLossBackward0>)\n",
            "epoch 45, loss 1.1268848180770874\n",
            "tensor(1.1199, grad_fn=<MseLossBackward0>)\n",
            "epoch 46, loss 1.1198617219924927\n",
            "tensor(1.1129, grad_fn=<MseLossBackward0>)\n",
            "epoch 47, loss 1.1128870248794556\n",
            "tensor(1.1060, grad_fn=<MseLossBackward0>)\n",
            "epoch 48, loss 1.1059601306915283\n",
            "tensor(1.0991, grad_fn=<MseLossBackward0>)\n",
            "epoch 49, loss 1.0990813970565796\n",
            "tensor(1.0922, grad_fn=<MseLossBackward0>)\n",
            "epoch 50, loss 1.0922496318817139\n",
            "tensor(1.0855, grad_fn=<MseLossBackward0>)\n",
            "epoch 51, loss 1.0854650735855103\n",
            "tensor(1.0787, grad_fn=<MseLossBackward0>)\n",
            "epoch 52, loss 1.0787272453308105\n",
            "tensor(1.0720, grad_fn=<MseLossBackward0>)\n",
            "epoch 53, loss 1.072035789489746\n",
            "tensor(1.0654, grad_fn=<MseLossBackward0>)\n",
            "epoch 54, loss 1.0653905868530273\n",
            "tensor(1.0588, grad_fn=<MseLossBackward0>)\n",
            "epoch 55, loss 1.0587910413742065\n",
            "tensor(1.0522, grad_fn=<MseLossBackward0>)\n",
            "epoch 56, loss 1.0522369146347046\n",
            "tensor(1.0457, grad_fn=<MseLossBackward0>)\n",
            "epoch 57, loss 1.0457278490066528\n",
            "tensor(1.0393, grad_fn=<MseLossBackward0>)\n",
            "epoch 58, loss 1.0392637252807617\n",
            "tensor(1.0328, grad_fn=<MseLossBackward0>)\n",
            "epoch 59, loss 1.032844066619873\n",
            "tensor(1.0265, grad_fn=<MseLossBackward0>)\n",
            "epoch 60, loss 1.0264687538146973\n",
            "tensor(1.0201, grad_fn=<MseLossBackward0>)\n",
            "epoch 61, loss 1.0201373100280762\n",
            "tensor(1.0138, grad_fn=<MseLossBackward0>)\n",
            "epoch 62, loss 1.0138493776321411\n",
            "tensor(1.0076, grad_fn=<MseLossBackward0>)\n",
            "epoch 63, loss 1.0076045989990234\n",
            "tensor(1.0014, grad_fn=<MseLossBackward0>)\n",
            "epoch 64, loss 1.0014032125473022\n",
            "tensor(0.9952, grad_fn=<MseLossBackward0>)\n",
            "epoch 65, loss 0.9952442646026611\n",
            "tensor(0.9891, grad_fn=<MseLossBackward0>)\n",
            "epoch 66, loss 0.9891278743743896\n",
            "tensor(0.9831, grad_fn=<MseLossBackward0>)\n",
            "epoch 67, loss 0.9830535650253296\n",
            "tensor(0.9770, grad_fn=<MseLossBackward0>)\n",
            "epoch 68, loss 0.9770210981369019\n",
            "tensor(0.9710, grad_fn=<MseLossBackward0>)\n",
            "epoch 69, loss 0.9710301160812378\n",
            "tensor(0.9651, grad_fn=<MseLossBackward0>)\n",
            "epoch 70, loss 0.9650803804397583\n",
            "tensor(0.9592, grad_fn=<MseLossBackward0>)\n",
            "epoch 71, loss 0.9591716527938843\n",
            "tensor(0.9533, grad_fn=<MseLossBackward0>)\n",
            "epoch 72, loss 0.9533036947250366\n",
            "tensor(0.9475, grad_fn=<MseLossBackward0>)\n",
            "epoch 73, loss 0.9474760890007019\n",
            "tensor(0.9417, grad_fn=<MseLossBackward0>)\n",
            "epoch 74, loss 0.9416887760162354\n",
            "tensor(0.9359, grad_fn=<MseLossBackward0>)\n",
            "epoch 75, loss 0.9359411001205444\n",
            "tensor(0.9302, grad_fn=<MseLossBackward0>)\n",
            "epoch 76, loss 0.9302330613136292\n",
            "tensor(0.9246, grad_fn=<MseLossBackward0>)\n",
            "epoch 77, loss 0.9245643019676208\n",
            "tensor(0.9189, grad_fn=<MseLossBackward0>)\n",
            "epoch 78, loss 0.9189346432685852\n",
            "tensor(0.9133, grad_fn=<MseLossBackward0>)\n",
            "epoch 79, loss 0.9133438467979431\n",
            "tensor(0.9078, grad_fn=<MseLossBackward0>)\n",
            "epoch 80, loss 0.9077914357185364\n",
            "tensor(0.9023, grad_fn=<MseLossBackward0>)\n",
            "epoch 81, loss 0.902277410030365\n",
            "tensor(0.8968, grad_fn=<MseLossBackward0>)\n",
            "epoch 82, loss 0.896801233291626\n",
            "tensor(0.8914, grad_fn=<MseLossBackward0>)\n",
            "epoch 83, loss 0.8913626670837402\n",
            "tensor(0.8860, grad_fn=<MseLossBackward0>)\n",
            "epoch 84, loss 0.8859618902206421\n",
            "tensor(0.8806, grad_fn=<MseLossBackward0>)\n",
            "epoch 85, loss 0.8805980086326599\n",
            "tensor(0.8753, grad_fn=<MseLossBackward0>)\n",
            "epoch 86, loss 0.8752710819244385\n",
            "tensor(0.8700, grad_fn=<MseLossBackward0>)\n",
            "epoch 87, loss 0.8699809908866882\n",
            "tensor(0.8647, grad_fn=<MseLossBackward0>)\n",
            "epoch 88, loss 0.864727258682251\n",
            "tensor(0.8595, grad_fn=<MseLossBackward0>)\n",
            "epoch 89, loss 0.8595096468925476\n",
            "tensor(0.8543, grad_fn=<MseLossBackward0>)\n",
            "epoch 90, loss 0.8543280959129333\n",
            "tensor(0.8492, grad_fn=<MseLossBackward0>)\n",
            "epoch 91, loss 0.8491822481155396\n",
            "tensor(0.8441, grad_fn=<MseLossBackward0>)\n",
            "epoch 92, loss 0.8440718054771423\n",
            "tensor(0.8390, grad_fn=<MseLossBackward0>)\n",
            "epoch 93, loss 0.8389964699745178\n",
            "tensor(0.8340, grad_fn=<MseLossBackward0>)\n",
            "epoch 94, loss 0.8339561820030212\n",
            "tensor(0.8290, grad_fn=<MseLossBackward0>)\n",
            "epoch 95, loss 0.8289504647254944\n",
            "tensor(0.8240, grad_fn=<MseLossBackward0>)\n",
            "epoch 96, loss 0.8239793181419373\n",
            "tensor(0.8190, grad_fn=<MseLossBackward0>)\n",
            "epoch 97, loss 0.819042444229126\n",
            "tensor(0.8141, grad_fn=<MseLossBackward0>)\n",
            "epoch 98, loss 0.8141396045684814\n",
            "tensor(0.8093, grad_fn=<MseLossBackward0>)\n",
            "epoch 99, loss 0.8092705011367798\n",
            "tensor(0.8044, grad_fn=<MseLossBackward0>)\n",
            "epoch 100, loss 0.8044348955154419\n",
            "tensor(0.7996, grad_fn=<MseLossBackward0>)\n",
            "epoch 101, loss 0.7996326684951782\n",
            "tensor(0.7949, grad_fn=<MseLossBackward0>)\n",
            "epoch 102, loss 0.7948633432388306\n",
            "tensor(0.7901, grad_fn=<MseLossBackward0>)\n",
            "epoch 103, loss 0.7901270389556885\n",
            "tensor(0.7854, grad_fn=<MseLossBackward0>)\n",
            "epoch 104, loss 0.7854232788085938\n",
            "tensor(0.7808, grad_fn=<MseLossBackward0>)\n",
            "epoch 105, loss 0.7807519435882568\n",
            "tensor(0.7761, grad_fn=<MseLossBackward0>)\n",
            "epoch 106, loss 0.7761127948760986\n",
            "tensor(0.7715, grad_fn=<MseLossBackward0>)\n",
            "epoch 107, loss 0.77150559425354\n",
            "tensor(0.7669, grad_fn=<MseLossBackward0>)\n",
            "epoch 108, loss 0.7669300436973572\n",
            "tensor(0.7624, grad_fn=<MseLossBackward0>)\n",
            "epoch 109, loss 0.7623860836029053\n",
            "tensor(0.7579, grad_fn=<MseLossBackward0>)\n",
            "epoch 110, loss 0.7578734159469604\n",
            "tensor(0.7534, grad_fn=<MseLossBackward0>)\n",
            "epoch 111, loss 0.7533918619155884\n",
            "tensor(0.7489, grad_fn=<MseLossBackward0>)\n",
            "epoch 112, loss 0.74894118309021\n",
            "tensor(0.7445, grad_fn=<MseLossBackward0>)\n",
            "epoch 113, loss 0.7445210218429565\n",
            "tensor(0.7401, grad_fn=<MseLossBackward0>)\n",
            "epoch 114, loss 0.7401314377784729\n",
            "tensor(0.7358, grad_fn=<MseLossBackward0>)\n",
            "epoch 115, loss 0.7357720136642456\n",
            "tensor(0.7314, grad_fn=<MseLossBackward0>)\n",
            "epoch 116, loss 0.7314425706863403\n",
            "tensor(0.7271, grad_fn=<MseLossBackward0>)\n",
            "epoch 117, loss 0.7271429896354675\n",
            "tensor(0.7229, grad_fn=<MseLossBackward0>)\n",
            "epoch 118, loss 0.7228730320930481\n",
            "tensor(0.7186, grad_fn=<MseLossBackward0>)\n",
            "epoch 119, loss 0.7186325192451477\n",
            "tensor(0.7144, grad_fn=<MseLossBackward0>)\n",
            "epoch 120, loss 0.7144212126731873\n",
            "tensor(0.7102, grad_fn=<MseLossBackward0>)\n",
            "epoch 121, loss 0.7102388143539429\n",
            "tensor(0.7061, grad_fn=<MseLossBackward0>)\n",
            "epoch 122, loss 0.7060854434967041\n",
            "tensor(0.7020, grad_fn=<MseLossBackward0>)\n",
            "epoch 123, loss 0.7019604444503784\n",
            "tensor(0.6979, grad_fn=<MseLossBackward0>)\n",
            "epoch 124, loss 0.6978639364242554\n",
            "tensor(0.6938, grad_fn=<MseLossBackward0>)\n",
            "epoch 125, loss 0.6937956213951111\n",
            "tensor(0.6898, grad_fn=<MseLossBackward0>)\n",
            "epoch 126, loss 0.689755380153656\n",
            "tensor(0.6857, grad_fn=<MseLossBackward0>)\n",
            "epoch 127, loss 0.6857429146766663\n",
            "tensor(0.6818, grad_fn=<MseLossBackward0>)\n",
            "epoch 128, loss 0.6817581653594971\n",
            "tensor(0.6778, grad_fn=<MseLossBackward0>)\n",
            "epoch 129, loss 0.6778007745742798\n",
            "tensor(0.6739, grad_fn=<MseLossBackward0>)\n",
            "epoch 130, loss 0.6738706231117249\n",
            "tensor(0.6700, grad_fn=<MseLossBackward0>)\n",
            "epoch 131, loss 0.6699675917625427\n",
            "tensor(0.6661, grad_fn=<MseLossBackward0>)\n",
            "epoch 132, loss 0.6660913825035095\n",
            "tensor(0.6622, grad_fn=<MseLossBackward0>)\n",
            "epoch 133, loss 0.6622418761253357\n",
            "tensor(0.6584, grad_fn=<MseLossBackward0>)\n",
            "epoch 134, loss 0.6584189534187317\n",
            "tensor(0.6546, grad_fn=<MseLossBackward0>)\n",
            "epoch 135, loss 0.6546223759651184\n",
            "tensor(0.6509, grad_fn=<MseLossBackward0>)\n",
            "epoch 136, loss 0.6508519053459167\n",
            "tensor(0.6471, grad_fn=<MseLossBackward0>)\n",
            "epoch 137, loss 0.6471073031425476\n",
            "tensor(0.6434, grad_fn=<MseLossBackward0>)\n",
            "epoch 138, loss 0.6433886289596558\n",
            "tensor(0.6397, grad_fn=<MseLossBackward0>)\n",
            "epoch 139, loss 0.6396955251693726\n",
            "tensor(0.6360, grad_fn=<MseLossBackward0>)\n",
            "epoch 140, loss 0.6360277533531189\n",
            "tensor(0.6324, grad_fn=<MseLossBackward0>)\n",
            "epoch 141, loss 0.6323853731155396\n",
            "tensor(0.6288, grad_fn=<MseLossBackward0>)\n",
            "epoch 142, loss 0.6287679672241211\n",
            "tensor(0.6252, grad_fn=<MseLossBackward0>)\n",
            "epoch 143, loss 0.6251757144927979\n",
            "tensor(0.6216, grad_fn=<MseLossBackward0>)\n",
            "epoch 144, loss 0.6216080188751221\n",
            "tensor(0.6181, grad_fn=<MseLossBackward0>)\n",
            "epoch 145, loss 0.6180648803710938\n",
            "tensor(0.6145, grad_fn=<MseLossBackward0>)\n",
            "epoch 146, loss 0.6145462393760681\n",
            "tensor(0.6111, grad_fn=<MseLossBackward0>)\n",
            "epoch 147, loss 0.6110516786575317\n",
            "tensor(0.6076, grad_fn=<MseLossBackward0>)\n",
            "epoch 148, loss 0.607581377029419\n",
            "tensor(0.6041, grad_fn=<MseLossBackward0>)\n",
            "epoch 149, loss 0.6041349172592163\n",
            "tensor(0.6007, grad_fn=<MseLossBackward0>)\n",
            "epoch 150, loss 0.6007120609283447\n",
            "tensor(0.5973, grad_fn=<MseLossBackward0>)\n",
            "epoch 151, loss 0.5973128080368042\n",
            "tensor(0.5939, grad_fn=<MseLossBackward0>)\n",
            "epoch 152, loss 0.59393709897995\n",
            "tensor(0.5906, grad_fn=<MseLossBackward0>)\n",
            "epoch 153, loss 0.5905845761299133\n",
            "tensor(0.5873, grad_fn=<MseLossBackward0>)\n",
            "epoch 154, loss 0.5872551202774048\n",
            "tensor(0.5839, grad_fn=<MseLossBackward0>)\n",
            "epoch 155, loss 0.58394855260849\n",
            "tensor(0.5807, grad_fn=<MseLossBackward0>)\n",
            "epoch 156, loss 0.5806649327278137\n",
            "tensor(0.5774, grad_fn=<MseLossBackward0>)\n",
            "epoch 157, loss 0.5774037837982178\n",
            "tensor(0.5742, grad_fn=<MseLossBackward0>)\n",
            "epoch 158, loss 0.5741651058197021\n",
            "tensor(0.5709, grad_fn=<MseLossBackward0>)\n",
            "epoch 159, loss 0.5709487795829773\n",
            "tensor(0.5678, grad_fn=<MseLossBackward0>)\n",
            "epoch 160, loss 0.5677545070648193\n",
            "tensor(0.5646, grad_fn=<MseLossBackward0>)\n",
            "epoch 161, loss 0.564582347869873\n",
            "tensor(0.5614, grad_fn=<MseLossBackward0>)\n",
            "epoch 162, loss 0.5614320039749146\n",
            "tensor(0.5583, grad_fn=<MseLossBackward0>)\n",
            "epoch 163, loss 0.5583034157752991\n",
            "tensor(0.5552, grad_fn=<MseLossBackward0>)\n",
            "epoch 164, loss 0.5551962852478027\n",
            "tensor(0.5521, grad_fn=<MseLossBackward0>)\n",
            "epoch 165, loss 0.5521106123924255\n",
            "tensor(0.5490, grad_fn=<MseLossBackward0>)\n",
            "epoch 166, loss 0.5490460395812988\n",
            "tensor(0.5460, grad_fn=<MseLossBackward0>)\n",
            "epoch 167, loss 0.5460027456283569\n",
            "tensor(0.5430, grad_fn=<MseLossBackward0>)\n",
            "epoch 168, loss 0.5429803729057312\n",
            "tensor(0.5400, grad_fn=<MseLossBackward0>)\n",
            "epoch 169, loss 0.5399787425994873\n",
            "tensor(0.5370, grad_fn=<MseLossBackward0>)\n",
            "epoch 170, loss 0.5369978547096252\n",
            "tensor(0.5340, grad_fn=<MseLossBackward0>)\n",
            "epoch 171, loss 0.5340375304222107\n",
            "tensor(0.5311, grad_fn=<MseLossBackward0>)\n",
            "epoch 172, loss 0.5310976505279541\n",
            "tensor(0.5282, grad_fn=<MseLossBackward0>)\n",
            "epoch 173, loss 0.5281778573989868\n",
            "tensor(0.5253, grad_fn=<MseLossBackward0>)\n",
            "epoch 174, loss 0.5252781510353088\n",
            "tensor(0.5224, grad_fn=<MseLossBackward0>)\n",
            "epoch 175, loss 0.5223984718322754\n",
            "tensor(0.5195, grad_fn=<MseLossBackward0>)\n",
            "epoch 176, loss 0.5195386409759521\n",
            "tensor(0.5167, grad_fn=<MseLossBackward0>)\n",
            "epoch 177, loss 0.5166985392570496\n",
            "tensor(0.5139, grad_fn=<MseLossBackward0>)\n",
            "epoch 178, loss 0.5138780474662781\n",
            "tensor(0.5111, grad_fn=<MseLossBackward0>)\n",
            "epoch 179, loss 0.511076807975769\n",
            "tensor(0.5083, grad_fn=<MseLossBackward0>)\n",
            "epoch 180, loss 0.5082950592041016\n",
            "tensor(0.5055, grad_fn=<MseLossBackward0>)\n",
            "epoch 181, loss 0.5055323839187622\n",
            "tensor(0.5028, grad_fn=<MseLossBackward0>)\n",
            "epoch 182, loss 0.5027887225151062\n",
            "tensor(0.5001, grad_fn=<MseLossBackward0>)\n",
            "epoch 183, loss 0.500063955783844\n",
            "tensor(0.4974, grad_fn=<MseLossBackward0>)\n",
            "epoch 184, loss 0.4973579943180084\n",
            "tensor(0.4947, grad_fn=<MseLossBackward0>)\n",
            "epoch 185, loss 0.4946705996990204\n",
            "tensor(0.4920, grad_fn=<MseLossBackward0>)\n",
            "epoch 186, loss 0.4920017421245575\n",
            "tensor(0.4894, grad_fn=<MseLossBackward0>)\n",
            "epoch 187, loss 0.4893513023853302\n",
            "tensor(0.4867, grad_fn=<MseLossBackward0>)\n",
            "epoch 188, loss 0.4867191016674042\n",
            "tensor(0.4841, grad_fn=<MseLossBackward0>)\n",
            "epoch 189, loss 0.48410505056381226\n",
            "tensor(0.4815, grad_fn=<MseLossBackward0>)\n",
            "epoch 190, loss 0.4815089702606201\n",
            "tensor(0.4789, grad_fn=<MseLossBackward0>)\n",
            "epoch 191, loss 0.4789307415485382\n",
            "tensor(0.4764, grad_fn=<MseLossBackward0>)\n",
            "epoch 192, loss 0.47637030482292175\n",
            "tensor(0.4738, grad_fn=<MseLossBackward0>)\n",
            "epoch 193, loss 0.4738275706768036\n",
            "tensor(0.4713, grad_fn=<MseLossBackward0>)\n",
            "epoch 194, loss 0.4713023006916046\n",
            "tensor(0.4688, grad_fn=<MseLossBackward0>)\n",
            "epoch 195, loss 0.4687943756580353\n",
            "tensor(0.4663, grad_fn=<MseLossBackward0>)\n",
            "epoch 196, loss 0.4663037955760956\n",
            "tensor(0.4638, grad_fn=<MseLossBackward0>)\n",
            "epoch 197, loss 0.46383029222488403\n",
            "tensor(0.4614, grad_fn=<MseLossBackward0>)\n",
            "epoch 198, loss 0.461373895406723\n",
            "tensor(0.4589, grad_fn=<MseLossBackward0>)\n",
            "epoch 199, loss 0.45893436670303345\n",
            "tensor(0.4565, grad_fn=<MseLossBackward0>)\n",
            "epoch 200, loss 0.4565116763114929\n",
            "tensor(0.4541, grad_fn=<MseLossBackward0>)\n",
            "epoch 201, loss 0.4541056454181671\n",
            "tensor(0.4517, grad_fn=<MseLossBackward0>)\n",
            "epoch 202, loss 0.45171618461608887\n",
            "tensor(0.4493, grad_fn=<MseLossBackward0>)\n",
            "epoch 203, loss 0.44934317469596863\n",
            "tensor(0.4470, grad_fn=<MseLossBackward0>)\n",
            "epoch 204, loss 0.44698646664619446\n",
            "tensor(0.4446, grad_fn=<MseLossBackward0>)\n",
            "epoch 205, loss 0.44464612007141113\n",
            "tensor(0.4423, grad_fn=<MseLossBackward0>)\n",
            "epoch 206, loss 0.44232177734375\n",
            "tensor(0.4400, grad_fn=<MseLossBackward0>)\n",
            "epoch 207, loss 0.4400135278701782\n",
            "tensor(0.4377, grad_fn=<MseLossBackward0>)\n",
            "epoch 208, loss 0.4377211034297943\n",
            "tensor(0.4354, grad_fn=<MseLossBackward0>)\n",
            "epoch 209, loss 0.43544453382492065\n",
            "tensor(0.4332, grad_fn=<MseLossBackward0>)\n",
            "epoch 210, loss 0.43318355083465576\n",
            "tensor(0.4309, grad_fn=<MseLossBackward0>)\n",
            "epoch 211, loss 0.430938184261322\n",
            "tensor(0.4287, grad_fn=<MseLossBackward0>)\n",
            "epoch 212, loss 0.4287082552909851\n",
            "tensor(0.4265, grad_fn=<MseLossBackward0>)\n",
            "epoch 213, loss 0.42649370431900024\n",
            "tensor(0.4243, grad_fn=<MseLossBackward0>)\n",
            "epoch 214, loss 0.42429447174072266\n",
            "tensor(0.4221, grad_fn=<MseLossBackward0>)\n",
            "epoch 215, loss 0.42211034893989563\n",
            "tensor(0.4199, grad_fn=<MseLossBackward0>)\n",
            "epoch 216, loss 0.4199412167072296\n",
            "tensor(0.4178, grad_fn=<MseLossBackward0>)\n",
            "epoch 217, loss 0.41778701543807983\n",
            "tensor(0.4156, grad_fn=<MseLossBackward0>)\n",
            "epoch 218, loss 0.4156476855278015\n",
            "tensor(0.4135, grad_fn=<MseLossBackward0>)\n",
            "epoch 219, loss 0.4135231077671051\n",
            "tensor(0.4114, grad_fn=<MseLossBackward0>)\n",
            "epoch 220, loss 0.4114131033420563\n",
            "tensor(0.4093, grad_fn=<MseLossBackward0>)\n",
            "epoch 221, loss 0.4093177616596222\n",
            "tensor(0.4072, grad_fn=<MseLossBackward0>)\n",
            "epoch 222, loss 0.4072367250919342\n",
            "tensor(0.4052, grad_fn=<MseLossBackward0>)\n",
            "epoch 223, loss 0.4051700830459595\n",
            "tensor(0.4031, grad_fn=<MseLossBackward0>)\n",
            "epoch 224, loss 0.40311768651008606\n",
            "tensor(0.4011, grad_fn=<MseLossBackward0>)\n",
            "epoch 225, loss 0.4010794758796692\n",
            "tensor(0.3991, grad_fn=<MseLossBackward0>)\n",
            "epoch 226, loss 0.399055153131485\n",
            "tensor(0.3970, grad_fn=<MseLossBackward0>)\n",
            "epoch 227, loss 0.397044837474823\n",
            "tensor(0.3950, grad_fn=<MseLossBackward0>)\n",
            "epoch 228, loss 0.3950483202934265\n",
            "tensor(0.3931, grad_fn=<MseLossBackward0>)\n",
            "epoch 229, loss 0.3930656611919403\n",
            "tensor(0.3911, grad_fn=<MseLossBackward0>)\n",
            "epoch 230, loss 0.39109665155410767\n",
            "tensor(0.3891, grad_fn=<MseLossBackward0>)\n",
            "epoch 231, loss 0.3891410827636719\n",
            "tensor(0.3872, grad_fn=<MseLossBackward0>)\n",
            "epoch 232, loss 0.3871990740299225\n",
            "tensor(0.3853, grad_fn=<MseLossBackward0>)\n",
            "epoch 233, loss 0.385270357131958\n",
            "tensor(0.3834, grad_fn=<MseLossBackward0>)\n",
            "epoch 234, loss 0.3833550214767456\n",
            "tensor(0.3815, grad_fn=<MseLossBackward0>)\n",
            "epoch 235, loss 0.38145285844802856\n",
            "tensor(0.3796, grad_fn=<MseLossBackward0>)\n",
            "epoch 236, loss 0.3795637786388397\n",
            "tensor(0.3777, grad_fn=<MseLossBackward0>)\n",
            "epoch 237, loss 0.3776877522468567\n",
            "tensor(0.3758, grad_fn=<MseLossBackward0>)\n",
            "epoch 238, loss 0.37582457065582275\n",
            "tensor(0.3740, grad_fn=<MseLossBackward0>)\n",
            "epoch 239, loss 0.3739742636680603\n",
            "tensor(0.3721, grad_fn=<MseLossBackward0>)\n",
            "epoch 240, loss 0.3721366822719574\n",
            "tensor(0.3703, grad_fn=<MseLossBackward0>)\n",
            "epoch 241, loss 0.3703117370605469\n",
            "tensor(0.3685, grad_fn=<MseLossBackward0>)\n",
            "epoch 242, loss 0.36849942803382874\n",
            "tensor(0.3667, grad_fn=<MseLossBackward0>)\n",
            "epoch 243, loss 0.36669954657554626\n",
            "tensor(0.3649, grad_fn=<MseLossBackward0>)\n",
            "epoch 244, loss 0.3649120032787323\n",
            "tensor(0.3631, grad_fn=<MseLossBackward0>)\n",
            "epoch 245, loss 0.3631368577480316\n",
            "tensor(0.3614, grad_fn=<MseLossBackward0>)\n",
            "epoch 246, loss 0.3613739609718323\n",
            "tensor(0.3596, grad_fn=<MseLossBackward0>)\n",
            "epoch 247, loss 0.35962316393852234\n",
            "tensor(0.3579, grad_fn=<MseLossBackward0>)\n",
            "epoch 248, loss 0.35788440704345703\n",
            "tensor(0.3562, grad_fn=<MseLossBackward0>)\n",
            "epoch 249, loss 0.35615769028663635\n",
            "tensor(0.3544, grad_fn=<MseLossBackward0>)\n",
            "epoch 250, loss 0.35444286465644836\n",
            "tensor(0.3527, grad_fn=<MseLossBackward0>)\n",
            "epoch 251, loss 0.3527398109436035\n",
            "tensor(0.3510, grad_fn=<MseLossBackward0>)\n",
            "epoch 252, loss 0.35104846954345703\n",
            "tensor(0.3494, grad_fn=<MseLossBackward0>)\n",
            "epoch 253, loss 0.34936875104904175\n",
            "tensor(0.3477, grad_fn=<MseLossBackward0>)\n",
            "epoch 254, loss 0.34770068526268005\n",
            "tensor(0.3460, grad_fn=<MseLossBackward0>)\n",
            "epoch 255, loss 0.34604406356811523\n",
            "tensor(0.3444, grad_fn=<MseLossBackward0>)\n",
            "epoch 256, loss 0.3443988859653473\n",
            "tensor(0.3428, grad_fn=<MseLossBackward0>)\n",
            "epoch 257, loss 0.34276503324508667\n",
            "tensor(0.3411, grad_fn=<MseLossBackward0>)\n",
            "epoch 258, loss 0.3411424160003662\n",
            "tensor(0.3395, grad_fn=<MseLossBackward0>)\n",
            "epoch 259, loss 0.33953097462654114\n",
            "tensor(0.3379, grad_fn=<MseLossBackward0>)\n",
            "epoch 260, loss 0.3379305899143219\n",
            "tensor(0.3363, grad_fn=<MseLossBackward0>)\n",
            "epoch 261, loss 0.33634132146835327\n",
            "tensor(0.3348, grad_fn=<MseLossBackward0>)\n",
            "epoch 262, loss 0.33476290106773376\n",
            "tensor(0.3332, grad_fn=<MseLossBackward0>)\n",
            "epoch 263, loss 0.33319535851478577\n",
            "tensor(0.3316, grad_fn=<MseLossBackward0>)\n",
            "epoch 264, loss 0.3316386640071869\n",
            "tensor(0.3301, grad_fn=<MseLossBackward0>)\n",
            "epoch 265, loss 0.3300926387310028\n",
            "tensor(0.3286, grad_fn=<MseLossBackward0>)\n",
            "epoch 266, loss 0.3285573124885559\n",
            "tensor(0.3270, grad_fn=<MseLossBackward0>)\n",
            "epoch 267, loss 0.32703253626823425\n",
            "tensor(0.3255, grad_fn=<MseLossBackward0>)\n",
            "epoch 268, loss 0.32551825046539307\n",
            "tensor(0.3240, grad_fn=<MseLossBackward0>)\n",
            "epoch 269, loss 0.32401445508003235\n",
            "tensor(0.3225, grad_fn=<MseLossBackward0>)\n",
            "epoch 270, loss 0.3225209414958954\n",
            "tensor(0.3210, grad_fn=<MseLossBackward0>)\n",
            "epoch 271, loss 0.32103779911994934\n",
            "tensor(0.3196, grad_fn=<MseLossBackward0>)\n",
            "epoch 272, loss 0.3195648193359375\n",
            "tensor(0.3181, grad_fn=<MseLossBackward0>)\n",
            "epoch 273, loss 0.3181019723415375\n",
            "tensor(0.3166, grad_fn=<MseLossBackward0>)\n",
            "epoch 274, loss 0.3166491985321045\n",
            "tensor(0.3152, grad_fn=<MseLossBackward0>)\n",
            "epoch 275, loss 0.3152064383029938\n",
            "tensor(0.3138, grad_fn=<MseLossBackward0>)\n",
            "epoch 276, loss 0.31377366185188293\n",
            "tensor(0.3124, grad_fn=<MseLossBackward0>)\n",
            "epoch 277, loss 0.31235069036483765\n",
            "tensor(0.3109, grad_fn=<MseLossBackward0>)\n",
            "epoch 278, loss 0.3109375238418579\n",
            "tensor(0.3095, grad_fn=<MseLossBackward0>)\n",
            "epoch 279, loss 0.30953410267829895\n",
            "tensor(0.3081, grad_fn=<MseLossBackward0>)\n",
            "epoch 280, loss 0.308140367269516\n",
            "tensor(0.3068, grad_fn=<MseLossBackward0>)\n",
            "epoch 281, loss 0.30675622820854187\n",
            "tensor(0.3054, grad_fn=<MseLossBackward0>)\n",
            "epoch 282, loss 0.3053816258907318\n",
            "tensor(0.3040, grad_fn=<MseLossBackward0>)\n",
            "epoch 283, loss 0.30401644110679626\n",
            "tensor(0.3027, grad_fn=<MseLossBackward0>)\n",
            "epoch 284, loss 0.30266073346138\n",
            "tensor(0.3013, grad_fn=<MseLossBackward0>)\n",
            "epoch 285, loss 0.30131420493125916\n",
            "tensor(0.3000, grad_fn=<MseLossBackward0>)\n",
            "epoch 286, loss 0.2999771535396576\n",
            "tensor(0.2986, grad_fn=<MseLossBackward0>)\n",
            "epoch 287, loss 0.2986491918563843\n",
            "tensor(0.2973, grad_fn=<MseLossBackward0>)\n",
            "epoch 288, loss 0.297330379486084\n",
            "tensor(0.2960, grad_fn=<MseLossBackward0>)\n",
            "epoch 289, loss 0.2960207164287567\n",
            "tensor(0.2947, grad_fn=<MseLossBackward0>)\n",
            "epoch 290, loss 0.2947200536727905\n",
            "tensor(0.2934, grad_fn=<MseLossBackward0>)\n",
            "epoch 291, loss 0.29342833161354065\n",
            "tensor(0.2921, grad_fn=<MseLossBackward0>)\n",
            "epoch 292, loss 0.2921455204486847\n",
            "tensor(0.2909, grad_fn=<MseLossBackward0>)\n",
            "epoch 293, loss 0.2908715307712555\n",
            "tensor(0.2896, grad_fn=<MseLossBackward0>)\n",
            "epoch 294, loss 0.28960633277893066\n",
            "tensor(0.2883, grad_fn=<MseLossBackward0>)\n",
            "epoch 295, loss 0.28834983706474304\n",
            "tensor(0.2871, grad_fn=<MseLossBackward0>)\n",
            "epoch 296, loss 0.28710198402404785\n",
            "tensor(0.2859, grad_fn=<MseLossBackward0>)\n",
            "epoch 297, loss 0.2858627438545227\n",
            "tensor(0.2846, grad_fn=<MseLossBackward0>)\n",
            "epoch 298, loss 0.28463202714920044\n",
            "tensor(0.2834, grad_fn=<MseLossBackward0>)\n",
            "epoch 299, loss 0.2834097445011139\n",
            "tensor(0.2822, grad_fn=<MseLossBackward0>)\n",
            "epoch 300, loss 0.28219592571258545\n",
            "tensor(0.2810, grad_fn=<MseLossBackward0>)\n",
            "epoch 301, loss 0.28099045157432556\n",
            "tensor(0.2798, grad_fn=<MseLossBackward0>)\n",
            "epoch 302, loss 0.27979329228401184\n",
            "tensor(0.2786, grad_fn=<MseLossBackward0>)\n",
            "epoch 303, loss 0.2786043584346771\n",
            "tensor(0.2774, grad_fn=<MseLossBackward0>)\n",
            "epoch 304, loss 0.277423620223999\n",
            "tensor(0.2763, grad_fn=<MseLossBackward0>)\n",
            "epoch 305, loss 0.2762509882450104\n",
            "tensor(0.2751, grad_fn=<MseLossBackward0>)\n",
            "epoch 306, loss 0.27508649230003357\n",
            "tensor(0.2739, grad_fn=<MseLossBackward0>)\n",
            "epoch 307, loss 0.27393001317977905\n",
            "tensor(0.2728, grad_fn=<MseLossBackward0>)\n",
            "epoch 308, loss 0.27278146147727966\n",
            "tensor(0.2716, grad_fn=<MseLossBackward0>)\n",
            "epoch 309, loss 0.271640807390213\n",
            "tensor(0.2705, grad_fn=<MseLossBackward0>)\n",
            "epoch 310, loss 0.2705080211162567\n",
            "tensor(0.2694, grad_fn=<MseLossBackward0>)\n",
            "epoch 311, loss 0.269383043050766\n",
            "tensor(0.2683, grad_fn=<MseLossBackward0>)\n",
            "epoch 312, loss 0.26826587319374084\n",
            "tensor(0.2672, grad_fn=<MseLossBackward0>)\n",
            "epoch 313, loss 0.26715636253356934\n",
            "tensor(0.2661, grad_fn=<MseLossBackward0>)\n",
            "epoch 314, loss 0.2660544812679291\n",
            "tensor(0.2650, grad_fn=<MseLossBackward0>)\n",
            "epoch 315, loss 0.2649601995944977\n",
            "tensor(0.2639, grad_fn=<MseLossBackward0>)\n",
            "epoch 316, loss 0.26387345790863037\n",
            "tensor(0.2628, grad_fn=<MseLossBackward0>)\n",
            "epoch 317, loss 0.26279416680336\n",
            "tensor(0.2617, grad_fn=<MseLossBackward0>)\n",
            "epoch 318, loss 0.26172223687171936\n",
            "tensor(0.2607, grad_fn=<MseLossBackward0>)\n",
            "epoch 319, loss 0.26065778732299805\n",
            "tensor(0.2596, grad_fn=<MseLossBackward0>)\n",
            "epoch 320, loss 0.2596006989479065\n",
            "tensor(0.2586, grad_fn=<MseLossBackward0>)\n",
            "epoch 321, loss 0.25855085253715515\n",
            "tensor(0.2575, grad_fn=<MseLossBackward0>)\n",
            "epoch 322, loss 0.2575082778930664\n",
            "tensor(0.2565, grad_fn=<MseLossBackward0>)\n",
            "epoch 323, loss 0.25647279620170593\n",
            "tensor(0.2554, grad_fn=<MseLossBackward0>)\n",
            "epoch 324, loss 0.2554445266723633\n",
            "tensor(0.2544, grad_fn=<MseLossBackward0>)\n",
            "epoch 325, loss 0.25442326068878174\n",
            "tensor(0.2534, grad_fn=<MseLossBackward0>)\n",
            "epoch 326, loss 0.25340908765792847\n",
            "tensor(0.2524, grad_fn=<MseLossBackward0>)\n",
            "epoch 327, loss 0.25240185856819153\n",
            "tensor(0.2514, grad_fn=<MseLossBackward0>)\n",
            "epoch 328, loss 0.2514016032218933\n",
            "tensor(0.2504, grad_fn=<MseLossBackward0>)\n",
            "epoch 329, loss 0.25040820240974426\n",
            "tensor(0.2494, grad_fn=<MseLossBackward0>)\n",
            "epoch 330, loss 0.24942170083522797\n",
            "tensor(0.2484, grad_fn=<MseLossBackward0>)\n",
            "epoch 331, loss 0.2484419345855713\n",
            "tensor(0.2475, grad_fn=<MseLossBackward0>)\n",
            "epoch 332, loss 0.24746891856193542\n",
            "tensor(0.2465, grad_fn=<MseLossBackward0>)\n",
            "epoch 333, loss 0.24650265276432037\n",
            "tensor(0.2455, grad_fn=<MseLossBackward0>)\n",
            "epoch 334, loss 0.2455430030822754\n",
            "tensor(0.2446, grad_fn=<MseLossBackward0>)\n",
            "epoch 335, loss 0.24458995461463928\n",
            "tensor(0.2436, grad_fn=<MseLossBackward0>)\n",
            "epoch 336, loss 0.24364350736141205\n",
            "tensor(0.2427, grad_fn=<MseLossBackward0>)\n",
            "epoch 337, loss 0.2427036017179489\n",
            "tensor(0.2418, grad_fn=<MseLossBackward0>)\n",
            "epoch 338, loss 0.24177010357379913\n",
            "tensor(0.2408, grad_fn=<MseLossBackward0>)\n",
            "epoch 339, loss 0.2408430576324463\n",
            "tensor(0.2399, grad_fn=<MseLossBackward0>)\n",
            "epoch 340, loss 0.23992237448692322\n",
            "tensor(0.2390, grad_fn=<MseLossBackward0>)\n",
            "epoch 341, loss 0.23900805413722992\n",
            "tensor(0.2381, grad_fn=<MseLossBackward0>)\n",
            "epoch 342, loss 0.2381000518798828\n",
            "tensor(0.2372, grad_fn=<MseLossBackward0>)\n",
            "epoch 343, loss 0.23719826340675354\n",
            "tensor(0.2363, grad_fn=<MseLossBackward0>)\n",
            "epoch 344, loss 0.23630273342132568\n",
            "tensor(0.2354, grad_fn=<MseLossBackward0>)\n",
            "epoch 345, loss 0.2354133278131485\n",
            "tensor(0.2345, grad_fn=<MseLossBackward0>)\n",
            "epoch 346, loss 0.23453007638454437\n",
            "tensor(0.2337, grad_fn=<MseLossBackward0>)\n",
            "epoch 347, loss 0.23365290462970734\n",
            "tensor(0.2328, grad_fn=<MseLossBackward0>)\n",
            "epoch 348, loss 0.2327817678451538\n",
            "tensor(0.2319, grad_fn=<MseLossBackward0>)\n",
            "epoch 349, loss 0.231916606426239\n",
            "tensor(0.2311, grad_fn=<MseLossBackward0>)\n",
            "epoch 350, loss 0.23105746507644653\n",
            "tensor(0.2302, grad_fn=<MseLossBackward0>)\n",
            "epoch 351, loss 0.23020420968532562\n",
            "tensor(0.2294, grad_fn=<MseLossBackward0>)\n",
            "epoch 352, loss 0.22935684025287628\n",
            "tensor(0.2285, grad_fn=<MseLossBackward0>)\n",
            "epoch 353, loss 0.22851531207561493\n",
            "tensor(0.2277, grad_fn=<MseLossBackward0>)\n",
            "epoch 354, loss 0.2276795506477356\n",
            "tensor(0.2268, grad_fn=<MseLossBackward0>)\n",
            "epoch 355, loss 0.22684957087039948\n",
            "tensor(0.2260, grad_fn=<MseLossBackward0>)\n",
            "epoch 356, loss 0.2260252833366394\n",
            "tensor(0.2252, grad_fn=<MseLossBackward0>)\n",
            "epoch 357, loss 0.225206658244133\n",
            "tensor(0.2244, grad_fn=<MseLossBackward0>)\n",
            "epoch 358, loss 0.22439371049404144\n",
            "tensor(0.2236, grad_fn=<MseLossBackward0>)\n",
            "epoch 359, loss 0.2235863357782364\n",
            "tensor(0.2228, grad_fn=<MseLossBackward0>)\n",
            "epoch 360, loss 0.22278453409671783\n",
            "tensor(0.2220, grad_fn=<MseLossBackward0>)\n",
            "epoch 361, loss 0.2219882607460022\n",
            "tensor(0.2212, grad_fn=<MseLossBackward0>)\n",
            "epoch 362, loss 0.2211974710226059\n",
            "tensor(0.2204, grad_fn=<MseLossBackward0>)\n",
            "epoch 363, loss 0.22041213512420654\n",
            "tensor(0.2196, grad_fn=<MseLossBackward0>)\n",
            "epoch 364, loss 0.21963216364383698\n",
            "tensor(0.2189, grad_fn=<MseLossBackward0>)\n",
            "epoch 365, loss 0.21885758638381958\n",
            "tensor(0.2181, grad_fn=<MseLossBackward0>)\n",
            "epoch 366, loss 0.21808834373950958\n",
            "tensor(0.2173, grad_fn=<MseLossBackward0>)\n",
            "epoch 367, loss 0.2173243761062622\n",
            "tensor(0.2166, grad_fn=<MseLossBackward0>)\n",
            "epoch 368, loss 0.21656574308872223\n",
            "tensor(0.2158, grad_fn=<MseLossBackward0>)\n",
            "epoch 369, loss 0.2158122956752777\n",
            "tensor(0.2151, grad_fn=<MseLossBackward0>)\n",
            "epoch 370, loss 0.21506404876708984\n",
            "tensor(0.2143, grad_fn=<MseLossBackward0>)\n",
            "epoch 371, loss 0.21432094275951385\n",
            "tensor(0.2136, grad_fn=<MseLossBackward0>)\n",
            "epoch 372, loss 0.21358293294906616\n",
            "tensor(0.2129, grad_fn=<MseLossBackward0>)\n",
            "epoch 373, loss 0.21285003423690796\n",
            "tensor(0.2121, grad_fn=<MseLossBackward0>)\n",
            "epoch 374, loss 0.2121221423149109\n",
            "tensor(0.2114, grad_fn=<MseLossBackward0>)\n",
            "epoch 375, loss 0.21139931678771973\n",
            "tensor(0.2107, grad_fn=<MseLossBackward0>)\n",
            "epoch 376, loss 0.21068143844604492\n",
            "tensor(0.2100, grad_fn=<MseLossBackward0>)\n",
            "epoch 377, loss 0.20996855199337006\n",
            "tensor(0.2093, grad_fn=<MseLossBackward0>)\n",
            "epoch 378, loss 0.20926053822040558\n",
            "tensor(0.2086, grad_fn=<MseLossBackward0>)\n",
            "epoch 379, loss 0.2085573971271515\n",
            "tensor(0.2079, grad_fn=<MseLossBackward0>)\n",
            "epoch 380, loss 0.2078590840101242\n",
            "tensor(0.2072, grad_fn=<MseLossBackward0>)\n",
            "epoch 381, loss 0.20716558396816254\n",
            "tensor(0.2065, grad_fn=<MseLossBackward0>)\n",
            "epoch 382, loss 0.20647689700126648\n",
            "tensor(0.2058, grad_fn=<MseLossBackward0>)\n",
            "epoch 383, loss 0.20579293370246887\n",
            "tensor(0.2051, grad_fn=<MseLossBackward0>)\n",
            "epoch 384, loss 0.20511367917060852\n",
            "tensor(0.2044, grad_fn=<MseLossBackward0>)\n",
            "epoch 385, loss 0.20443910360336304\n",
            "tensor(0.2038, grad_fn=<MseLossBackward0>)\n",
            "epoch 386, loss 0.20376920700073242\n",
            "tensor(0.2031, grad_fn=<MseLossBackward0>)\n",
            "epoch 387, loss 0.2031038999557495\n",
            "tensor(0.2024, grad_fn=<MseLossBackward0>)\n",
            "epoch 388, loss 0.20244313776493073\n",
            "tensor(0.2018, grad_fn=<MseLossBackward0>)\n",
            "epoch 389, loss 0.20178693532943726\n",
            "tensor(0.2011, grad_fn=<MseLossBackward0>)\n",
            "epoch 390, loss 0.2011352926492691\n",
            "tensor(0.2005, grad_fn=<MseLossBackward0>)\n",
            "epoch 391, loss 0.2004881054162979\n",
            "tensor(0.1998, grad_fn=<MseLossBackward0>)\n",
            "epoch 392, loss 0.19984541833400726\n",
            "tensor(0.1992, grad_fn=<MseLossBackward0>)\n",
            "epoch 393, loss 0.1992071121931076\n",
            "tensor(0.1986, grad_fn=<MseLossBackward0>)\n",
            "epoch 394, loss 0.19857320189476013\n",
            "tensor(0.1979, grad_fn=<MseLossBackward0>)\n",
            "epoch 395, loss 0.19794368743896484\n",
            "tensor(0.1973, grad_fn=<MseLossBackward0>)\n",
            "epoch 396, loss 0.19731847941875458\n",
            "tensor(0.1967, grad_fn=<MseLossBackward0>)\n",
            "epoch 397, loss 0.19669760763645172\n",
            "tensor(0.1961, grad_fn=<MseLossBackward0>)\n",
            "epoch 398, loss 0.1960809826850891\n",
            "tensor(0.1955, grad_fn=<MseLossBackward0>)\n",
            "epoch 399, loss 0.19546858966350555\n",
            "tensor(0.1949, grad_fn=<MseLossBackward0>)\n",
            "epoch 400, loss 0.19486045837402344\n",
            "tensor(0.1943, grad_fn=<MseLossBackward0>)\n",
            "epoch 401, loss 0.1942564994096756\n",
            "tensor(0.1937, grad_fn=<MseLossBackward0>)\n",
            "epoch 402, loss 0.19365669786930084\n",
            "tensor(0.1931, grad_fn=<MseLossBackward0>)\n",
            "epoch 403, loss 0.1930610090494156\n",
            "tensor(0.1925, grad_fn=<MseLossBackward0>)\n",
            "epoch 404, loss 0.19246943295001984\n",
            "tensor(0.1919, grad_fn=<MseLossBackward0>)\n",
            "epoch 405, loss 0.1918819099664688\n",
            "tensor(0.1913, grad_fn=<MseLossBackward0>)\n",
            "epoch 406, loss 0.1912984997034073\n",
            "tensor(0.1907, grad_fn=<MseLossBackward0>)\n",
            "epoch 407, loss 0.19071906805038452\n",
            "tensor(0.1901, grad_fn=<MseLossBackward0>)\n",
            "epoch 408, loss 0.1901436448097229\n",
            "tensor(0.1896, grad_fn=<MseLossBackward0>)\n",
            "epoch 409, loss 0.18957215547561646\n",
            "tensor(0.1890, grad_fn=<MseLossBackward0>)\n",
            "epoch 410, loss 0.18900462985038757\n",
            "tensor(0.1884, grad_fn=<MseLossBackward0>)\n",
            "epoch 411, loss 0.18844099342823029\n",
            "tensor(0.1879, grad_fn=<MseLossBackward0>)\n",
            "epoch 412, loss 0.1878812164068222\n",
            "tensor(0.1873, grad_fn=<MseLossBackward0>)\n",
            "epoch 413, loss 0.18732532858848572\n",
            "tensor(0.1868, grad_fn=<MseLossBackward0>)\n",
            "epoch 414, loss 0.18677328526973724\n",
            "tensor(0.1862, grad_fn=<MseLossBackward0>)\n",
            "epoch 415, loss 0.186225026845932\n",
            "tensor(0.1857, grad_fn=<MseLossBackward0>)\n",
            "epoch 416, loss 0.18568052351474762\n",
            "tensor(0.1851, grad_fn=<MseLossBackward0>)\n",
            "epoch 417, loss 0.18513981997966766\n",
            "tensor(0.1846, grad_fn=<MseLossBackward0>)\n",
            "epoch 418, loss 0.1846027821302414\n",
            "tensor(0.1841, grad_fn=<MseLossBackward0>)\n",
            "epoch 419, loss 0.1840694695711136\n",
            "tensor(0.1835, grad_fn=<MseLossBackward0>)\n",
            "epoch 420, loss 0.18353985249996185\n",
            "tensor(0.1830, grad_fn=<MseLossBackward0>)\n",
            "epoch 421, loss 0.18301384150981903\n",
            "tensor(0.1825, grad_fn=<MseLossBackward0>)\n",
            "epoch 422, loss 0.1824914664030075\n",
            "tensor(0.1820, grad_fn=<MseLossBackward0>)\n",
            "epoch 423, loss 0.1819727122783661\n",
            "tensor(0.1815, grad_fn=<MseLossBackward0>)\n",
            "epoch 424, loss 0.1814575046300888\n",
            "tensor(0.1809, grad_fn=<MseLossBackward0>)\n",
            "epoch 425, loss 0.18094587326049805\n",
            "tensor(0.1804, grad_fn=<MseLossBackward0>)\n",
            "epoch 426, loss 0.18043774366378784\n",
            "tensor(0.1799, grad_fn=<MseLossBackward0>)\n",
            "epoch 427, loss 0.1799330711364746\n",
            "tensor(0.1794, grad_fn=<MseLossBackward0>)\n",
            "epoch 428, loss 0.17943193018436432\n",
            "tensor(0.1789, grad_fn=<MseLossBackward0>)\n",
            "epoch 429, loss 0.1789342612028122\n",
            "tensor(0.1784, grad_fn=<MseLossBackward0>)\n",
            "epoch 430, loss 0.17843998968601227\n",
            "tensor(0.1779, grad_fn=<MseLossBackward0>)\n",
            "epoch 431, loss 0.17794911563396454\n",
            "tensor(0.1775, grad_fn=<MseLossBackward0>)\n",
            "epoch 432, loss 0.1774616241455078\n",
            "tensor(0.1770, grad_fn=<MseLossBackward0>)\n",
            "epoch 433, loss 0.1769775152206421\n",
            "tensor(0.1765, grad_fn=<MseLossBackward0>)\n",
            "epoch 434, loss 0.1764966994524002\n",
            "tensor(0.1760, grad_fn=<MseLossBackward0>)\n",
            "epoch 435, loss 0.17601922154426575\n",
            "tensor(0.1755, grad_fn=<MseLossBackward0>)\n",
            "epoch 436, loss 0.17554500699043274\n",
            "tensor(0.1751, grad_fn=<MseLossBackward0>)\n",
            "epoch 437, loss 0.17507408559322357\n",
            "tensor(0.1746, grad_fn=<MseLossBackward0>)\n",
            "epoch 438, loss 0.17460639774799347\n",
            "tensor(0.1741, grad_fn=<MseLossBackward0>)\n",
            "epoch 439, loss 0.17414194345474243\n",
            "tensor(0.1737, grad_fn=<MseLossBackward0>)\n",
            "epoch 440, loss 0.17368067800998688\n",
            "tensor(0.1732, grad_fn=<MseLossBackward0>)\n",
            "epoch 441, loss 0.1732225865125656\n",
            "tensor(0.1728, grad_fn=<MseLossBackward0>)\n",
            "epoch 442, loss 0.17276765406131744\n",
            "tensor(0.1723, grad_fn=<MseLossBackward0>)\n",
            "epoch 443, loss 0.17231586575508118\n",
            "tensor(0.1719, grad_fn=<MseLossBackward0>)\n",
            "epoch 444, loss 0.17186717689037323\n",
            "tensor(0.1714, grad_fn=<MseLossBackward0>)\n",
            "epoch 445, loss 0.1714215725660324\n",
            "tensor(0.1710, grad_fn=<MseLossBackward0>)\n",
            "epoch 446, loss 0.17097902297973633\n",
            "tensor(0.1705, grad_fn=<MseLossBackward0>)\n",
            "epoch 447, loss 0.17053954303264618\n",
            "tensor(0.1701, grad_fn=<MseLossBackward0>)\n",
            "epoch 448, loss 0.17010310292243958\n",
            "tensor(0.1697, grad_fn=<MseLossBackward0>)\n",
            "epoch 449, loss 0.16966964304447174\n",
            "tensor(0.1692, grad_fn=<MseLossBackward0>)\n",
            "epoch 450, loss 0.16923917829990387\n",
            "tensor(0.1688, grad_fn=<MseLossBackward0>)\n",
            "epoch 451, loss 0.16881166398525238\n",
            "tensor(0.1684, grad_fn=<MseLossBackward0>)\n",
            "epoch 452, loss 0.16838712990283966\n",
            "tensor(0.1680, grad_fn=<MseLossBackward0>)\n",
            "epoch 453, loss 0.16796551644802094\n",
            "tensor(0.1675, grad_fn=<MseLossBackward0>)\n",
            "epoch 454, loss 0.16754676401615143\n",
            "tensor(0.1671, grad_fn=<MseLossBackward0>)\n",
            "epoch 455, loss 0.16713091731071472\n",
            "tensor(0.1667, grad_fn=<MseLossBackward0>)\n",
            "epoch 456, loss 0.16671791672706604\n",
            "tensor(0.1663, grad_fn=<MseLossBackward0>)\n",
            "epoch 457, loss 0.16630782186985016\n",
            "tensor(0.1659, grad_fn=<MseLossBackward0>)\n",
            "epoch 458, loss 0.16590049862861633\n",
            "tensor(0.1655, grad_fn=<MseLossBackward0>)\n",
            "epoch 459, loss 0.16549602150917053\n",
            "tensor(0.1651, grad_fn=<MseLossBackward0>)\n",
            "epoch 460, loss 0.1650943011045456\n",
            "tensor(0.1647, grad_fn=<MseLossBackward0>)\n",
            "epoch 461, loss 0.16469533741474152\n",
            "tensor(0.1643, grad_fn=<MseLossBackward0>)\n",
            "epoch 462, loss 0.1642991453409195\n",
            "tensor(0.1639, grad_fn=<MseLossBackward0>)\n",
            "epoch 463, loss 0.16390566527843475\n",
            "tensor(0.1635, grad_fn=<MseLossBackward0>)\n",
            "epoch 464, loss 0.1635149121284485\n",
            "tensor(0.1631, grad_fn=<MseLossBackward0>)\n",
            "epoch 465, loss 0.16312682628631592\n",
            "tensor(0.1627, grad_fn=<MseLossBackward0>)\n",
            "epoch 466, loss 0.16274142265319824\n",
            "tensor(0.1624, grad_fn=<MseLossBackward0>)\n",
            "epoch 467, loss 0.16235867142677307\n",
            "tensor(0.1620, grad_fn=<MseLossBackward0>)\n",
            "epoch 468, loss 0.1619785726070404\n",
            "tensor(0.1616, grad_fn=<MseLossBackward0>)\n",
            "epoch 469, loss 0.16160106658935547\n",
            "tensor(0.1612, grad_fn=<MseLossBackward0>)\n",
            "epoch 470, loss 0.16122619807720184\n",
            "tensor(0.1609, grad_fn=<MseLossBackward0>)\n",
            "epoch 471, loss 0.16085386276245117\n",
            "tensor(0.1605, grad_fn=<MseLossBackward0>)\n",
            "epoch 472, loss 0.16048412024974823\n",
            "tensor(0.1601, grad_fn=<MseLossBackward0>)\n",
            "epoch 473, loss 0.16011692583560944\n",
            "tensor(0.1598, grad_fn=<MseLossBackward0>)\n",
            "epoch 474, loss 0.1597522348165512\n",
            "tensor(0.1594, grad_fn=<MseLossBackward0>)\n",
            "epoch 475, loss 0.15939009189605713\n",
            "tensor(0.1590, grad_fn=<MseLossBackward0>)\n",
            "epoch 476, loss 0.15903043746948242\n",
            "tensor(0.1587, grad_fn=<MseLossBackward0>)\n",
            "epoch 477, loss 0.1586732417345047\n",
            "tensor(0.1583, grad_fn=<MseLossBackward0>)\n",
            "epoch 478, loss 0.15831851959228516\n",
            "tensor(0.1580, grad_fn=<MseLossBackward0>)\n",
            "epoch 479, loss 0.1579662263393402\n",
            "tensor(0.1576, grad_fn=<MseLossBackward0>)\n",
            "epoch 480, loss 0.15761637687683105\n",
            "tensor(0.1573, grad_fn=<MseLossBackward0>)\n",
            "epoch 481, loss 0.15726891160011292\n",
            "tensor(0.1569, grad_fn=<MseLossBackward0>)\n",
            "epoch 482, loss 0.15692389011383057\n",
            "tensor(0.1566, grad_fn=<MseLossBackward0>)\n",
            "epoch 483, loss 0.15658119320869446\n",
            "tensor(0.1562, grad_fn=<MseLossBackward0>)\n",
            "epoch 484, loss 0.15624089539051056\n",
            "tensor(0.1559, grad_fn=<MseLossBackward0>)\n",
            "epoch 485, loss 0.1559029221534729\n",
            "tensor(0.1556, grad_fn=<MseLossBackward0>)\n",
            "epoch 486, loss 0.1555672585964203\n",
            "tensor(0.1552, grad_fn=<MseLossBackward0>)\n",
            "epoch 487, loss 0.15523391962051392\n",
            "tensor(0.1549, grad_fn=<MseLossBackward0>)\n",
            "epoch 488, loss 0.15490290522575378\n",
            "tensor(0.1546, grad_fn=<MseLossBackward0>)\n",
            "epoch 489, loss 0.15457412600517273\n",
            "tensor(0.1542, grad_fn=<MseLossBackward0>)\n",
            "epoch 490, loss 0.15424764156341553\n",
            "tensor(0.1539, grad_fn=<MseLossBackward0>)\n",
            "epoch 491, loss 0.1539234071969986\n",
            "tensor(0.1536, grad_fn=<MseLossBackward0>)\n",
            "epoch 492, loss 0.15360140800476074\n",
            "tensor(0.1533, grad_fn=<MseLossBackward0>)\n",
            "epoch 493, loss 0.15328159928321838\n",
            "tensor(0.1530, grad_fn=<MseLossBackward0>)\n",
            "epoch 494, loss 0.1529640257358551\n",
            "tensor(0.1526, grad_fn=<MseLossBackward0>)\n",
            "epoch 495, loss 0.15264859795570374\n",
            "tensor(0.1523, grad_fn=<MseLossBackward0>)\n",
            "epoch 496, loss 0.15233534574508667\n",
            "tensor(0.1520, grad_fn=<MseLossBackward0>)\n",
            "epoch 497, loss 0.1520242691040039\n",
            "tensor(0.1517, grad_fn=<MseLossBackward0>)\n",
            "epoch 498, loss 0.15171535313129425\n",
            "tensor(0.1514, grad_fn=<MseLossBackward0>)\n",
            "epoch 499, loss 0.15140855312347412\n",
            "tensor(0.1511, grad_fn=<MseLossBackward0>)\n",
            "epoch 500, loss 0.15110386908054352\n",
            "tensor(0.1508, grad_fn=<MseLossBackward0>)\n",
            "epoch 501, loss 0.15080128610134125\n",
            "tensor(0.1505, grad_fn=<MseLossBackward0>)\n",
            "epoch 502, loss 0.15050077438354492\n",
            "tensor(0.1502, grad_fn=<MseLossBackward0>)\n",
            "epoch 503, loss 0.15020237863063812\n",
            "tensor(0.1499, grad_fn=<MseLossBackward0>)\n",
            "epoch 504, loss 0.1499059498310089\n",
            "tensor(0.1496, grad_fn=<MseLossBackward0>)\n",
            "epoch 505, loss 0.14961163699626923\n",
            "tensor(0.1493, grad_fn=<MseLossBackward0>)\n",
            "epoch 506, loss 0.1493193358182907\n",
            "tensor(0.1490, grad_fn=<MseLossBackward0>)\n",
            "epoch 507, loss 0.14902901649475098\n",
            "tensor(0.1487, grad_fn=<MseLossBackward0>)\n",
            "epoch 508, loss 0.14874067902565002\n",
            "tensor(0.1485, grad_fn=<MseLossBackward0>)\n",
            "epoch 509, loss 0.14845438301563263\n",
            "tensor(0.1482, grad_fn=<MseLossBackward0>)\n",
            "epoch 510, loss 0.14817003905773163\n",
            "tensor(0.1479, grad_fn=<MseLossBackward0>)\n",
            "epoch 511, loss 0.14788763225078583\n",
            "tensor(0.1476, grad_fn=<MseLossBackward0>)\n",
            "epoch 512, loss 0.14760714769363403\n",
            "tensor(0.1473, grad_fn=<MseLossBackward0>)\n",
            "epoch 513, loss 0.14732864499092102\n",
            "tensor(0.1471, grad_fn=<MseLossBackward0>)\n",
            "epoch 514, loss 0.14705206453800201\n",
            "tensor(0.1468, grad_fn=<MseLossBackward0>)\n",
            "epoch 515, loss 0.14677734673023224\n",
            "tensor(0.1465, grad_fn=<MseLossBackward0>)\n",
            "epoch 516, loss 0.14650456607341766\n",
            "tensor(0.1462, grad_fn=<MseLossBackward0>)\n",
            "epoch 517, loss 0.14623361825942993\n",
            "tensor(0.1460, grad_fn=<MseLossBackward0>)\n",
            "epoch 518, loss 0.145964577794075\n",
            "tensor(0.1457, grad_fn=<MseLossBackward0>)\n",
            "epoch 519, loss 0.14569739997386932\n",
            "tensor(0.1454, grad_fn=<MseLossBackward0>)\n",
            "epoch 520, loss 0.14543204009532928\n",
            "tensor(0.1452, grad_fn=<MseLossBackward0>)\n",
            "epoch 521, loss 0.1451684832572937\n",
            "tensor(0.1449, grad_fn=<MseLossBackward0>)\n",
            "epoch 522, loss 0.14490678906440735\n",
            "tensor(0.1446, grad_fn=<MseLossBackward0>)\n",
            "epoch 523, loss 0.14464691281318665\n",
            "tensor(0.1444, grad_fn=<MseLossBackward0>)\n",
            "epoch 524, loss 0.14438877999782562\n",
            "tensor(0.1441, grad_fn=<MseLossBackward0>)\n",
            "epoch 525, loss 0.14413243532180786\n",
            "tensor(0.1439, grad_fn=<MseLossBackward0>)\n",
            "epoch 526, loss 0.14387783408164978\n",
            "tensor(0.1436, grad_fn=<MseLossBackward0>)\n",
            "epoch 527, loss 0.14362503588199615\n",
            "tensor(0.1434, grad_fn=<MseLossBackward0>)\n",
            "epoch 528, loss 0.14337393641471863\n",
            "tensor(0.1431, grad_fn=<MseLossBackward0>)\n",
            "epoch 529, loss 0.14312459528446198\n",
            "tensor(0.1429, grad_fn=<MseLossBackward0>)\n",
            "epoch 530, loss 0.14287696778774261\n",
            "tensor(0.1426, grad_fn=<MseLossBackward0>)\n",
            "epoch 531, loss 0.14263099431991577\n",
            "tensor(0.1424, grad_fn=<MseLossBackward0>)\n",
            "epoch 532, loss 0.1423867791891098\n",
            "tensor(0.1421, grad_fn=<MseLossBackward0>)\n",
            "epoch 533, loss 0.14214420318603516\n",
            "tensor(0.1419, grad_fn=<MseLossBackward0>)\n",
            "epoch 534, loss 0.14190329611301422\n",
            "tensor(0.1417, grad_fn=<MseLossBackward0>)\n",
            "epoch 535, loss 0.14166408777236938\n",
            "tensor(0.1414, grad_fn=<MseLossBackward0>)\n",
            "epoch 536, loss 0.14142650365829468\n",
            "tensor(0.1412, grad_fn=<MseLossBackward0>)\n",
            "epoch 537, loss 0.1411905586719513\n",
            "tensor(0.1410, grad_fn=<MseLossBackward0>)\n",
            "epoch 538, loss 0.14095623791217804\n",
            "tensor(0.1407, grad_fn=<MseLossBackward0>)\n",
            "epoch 539, loss 0.1407235562801361\n",
            "tensor(0.1405, grad_fn=<MseLossBackward0>)\n",
            "epoch 540, loss 0.14049243927001953\n",
            "tensor(0.1403, grad_fn=<MseLossBackward0>)\n",
            "epoch 541, loss 0.14026297628879547\n",
            "tensor(0.1400, grad_fn=<MseLossBackward0>)\n",
            "epoch 542, loss 0.140035018324852\n",
            "tensor(0.1398, grad_fn=<MseLossBackward0>)\n",
            "epoch 543, loss 0.13980866968631744\n",
            "tensor(0.1396, grad_fn=<MseLossBackward0>)\n",
            "epoch 544, loss 0.13958385586738586\n",
            "tensor(0.1394, grad_fn=<MseLossBackward0>)\n",
            "epoch 545, loss 0.13936059176921844\n",
            "tensor(0.1391, grad_fn=<MseLossBackward0>)\n",
            "epoch 546, loss 0.139138862490654\n",
            "tensor(0.1389, grad_fn=<MseLossBackward0>)\n",
            "epoch 547, loss 0.1389186829328537\n",
            "tensor(0.1387, grad_fn=<MseLossBackward0>)\n",
            "epoch 548, loss 0.13870002329349518\n",
            "tensor(0.1385, grad_fn=<MseLossBackward0>)\n",
            "epoch 549, loss 0.13848286867141724\n",
            "tensor(0.1383, grad_fn=<MseLossBackward0>)\n",
            "epoch 550, loss 0.1382671594619751\n",
            "tensor(0.1381, grad_fn=<MseLossBackward0>)\n",
            "epoch 551, loss 0.13805298507213593\n",
            "tensor(0.1378, grad_fn=<MseLossBackward0>)\n",
            "epoch 552, loss 0.13784024119377136\n",
            "tensor(0.1376, grad_fn=<MseLossBackward0>)\n",
            "epoch 553, loss 0.13762901723384857\n",
            "tensor(0.1374, grad_fn=<MseLossBackward0>)\n",
            "epoch 554, loss 0.1374192237854004\n",
            "tensor(0.1372, grad_fn=<MseLossBackward0>)\n",
            "epoch 555, loss 0.137210875749588\n",
            "tensor(0.1370, grad_fn=<MseLossBackward0>)\n",
            "epoch 556, loss 0.13700395822525024\n",
            "tensor(0.1368, grad_fn=<MseLossBackward0>)\n",
            "epoch 557, loss 0.13679848611354828\n",
            "tensor(0.1366, grad_fn=<MseLossBackward0>)\n",
            "epoch 558, loss 0.13659439980983734\n",
            "tensor(0.1364, grad_fn=<MseLossBackward0>)\n",
            "epoch 559, loss 0.1363917589187622\n",
            "tensor(0.1362, grad_fn=<MseLossBackward0>)\n",
            "epoch 560, loss 0.1361904740333557\n",
            "tensor(0.1360, grad_fn=<MseLossBackward0>)\n",
            "epoch 561, loss 0.13599060475826263\n",
            "tensor(0.1358, grad_fn=<MseLossBackward0>)\n",
            "epoch 562, loss 0.1357920914888382\n",
            "tensor(0.1356, grad_fn=<MseLossBackward0>)\n",
            "epoch 563, loss 0.1355949342250824\n",
            "tensor(0.1354, grad_fn=<MseLossBackward0>)\n",
            "epoch 564, loss 0.13539914786815643\n",
            "tensor(0.1352, grad_fn=<MseLossBackward0>)\n",
            "epoch 565, loss 0.13520470261573792\n",
            "tensor(0.1350, grad_fn=<MseLossBackward0>)\n",
            "epoch 566, loss 0.13501161336898804\n",
            "tensor(0.1348, grad_fn=<MseLossBackward0>)\n",
            "epoch 567, loss 0.13481982052326202\n",
            "tensor(0.1346, grad_fn=<MseLossBackward0>)\n",
            "epoch 568, loss 0.13462939858436584\n",
            "tensor(0.1344, grad_fn=<MseLossBackward0>)\n",
            "epoch 569, loss 0.13444025814533234\n",
            "tensor(0.1343, grad_fn=<MseLossBackward0>)\n",
            "epoch 570, loss 0.13425244390964508\n",
            "tensor(0.1341, grad_fn=<MseLossBackward0>)\n",
            "epoch 571, loss 0.1340659111738205\n",
            "tensor(0.1339, grad_fn=<MseLossBackward0>)\n",
            "epoch 572, loss 0.13388067483901978\n",
            "tensor(0.1337, grad_fn=<MseLossBackward0>)\n",
            "epoch 573, loss 0.13369667530059814\n",
            "tensor(0.1335, grad_fn=<MseLossBackward0>)\n",
            "epoch 574, loss 0.13351400196552277\n",
            "tensor(0.1333, grad_fn=<MseLossBackward0>)\n",
            "epoch 575, loss 0.1333325356245041\n",
            "tensor(0.1332, grad_fn=<MseLossBackward0>)\n",
            "epoch 576, loss 0.1331523358821869\n",
            "tensor(0.1330, grad_fn=<MseLossBackward0>)\n",
            "epoch 577, loss 0.13297338783740997\n",
            "tensor(0.1328, grad_fn=<MseLossBackward0>)\n",
            "epoch 578, loss 0.13279564678668976\n",
            "tensor(0.1326, grad_fn=<MseLossBackward0>)\n",
            "epoch 579, loss 0.13261917233467102\n",
            "tensor(0.1324, grad_fn=<MseLossBackward0>)\n",
            "epoch 580, loss 0.1324438750743866\n",
            "tensor(0.1323, grad_fn=<MseLossBackward0>)\n",
            "epoch 581, loss 0.13226979970932007\n",
            "tensor(0.1321, grad_fn=<MseLossBackward0>)\n",
            "epoch 582, loss 0.13209693133831024\n",
            "tensor(0.1319, grad_fn=<MseLossBackward0>)\n",
            "epoch 583, loss 0.13192525506019592\n",
            "tensor(0.1318, grad_fn=<MseLossBackward0>)\n",
            "epoch 584, loss 0.13175471127033234\n",
            "tensor(0.1316, grad_fn=<MseLossBackward0>)\n",
            "epoch 585, loss 0.13158538937568665\n",
            "tensor(0.1314, grad_fn=<MseLossBackward0>)\n",
            "epoch 586, loss 0.13141725957393646\n",
            "tensor(0.1313, grad_fn=<MseLossBackward0>)\n",
            "epoch 587, loss 0.13125021755695343\n",
            "tensor(0.1311, grad_fn=<MseLossBackward0>)\n",
            "epoch 588, loss 0.1310843825340271\n",
            "tensor(0.1309, grad_fn=<MseLossBackward0>)\n",
            "epoch 589, loss 0.1309196501970291\n",
            "tensor(0.1308, grad_fn=<MseLossBackward0>)\n",
            "epoch 590, loss 0.13075609505176544\n",
            "tensor(0.1306, grad_fn=<MseLossBackward0>)\n",
            "epoch 591, loss 0.13059362769126892\n",
            "tensor(0.1304, grad_fn=<MseLossBackward0>)\n",
            "epoch 592, loss 0.13043229281902313\n",
            "tensor(0.1303, grad_fn=<MseLossBackward0>)\n",
            "epoch 593, loss 0.1302720457315445\n",
            "tensor(0.1301, grad_fn=<MseLossBackward0>)\n",
            "epoch 594, loss 0.1301129311323166\n",
            "tensor(0.1300, grad_fn=<MseLossBackward0>)\n",
            "epoch 595, loss 0.12995491921901703\n",
            "tensor(0.1298, grad_fn=<MseLossBackward0>)\n",
            "epoch 596, loss 0.12979799509048462\n",
            "tensor(0.1296, grad_fn=<MseLossBackward0>)\n",
            "epoch 597, loss 0.12964215874671936\n",
            "tensor(0.1295, grad_fn=<MseLossBackward0>)\n",
            "epoch 598, loss 0.12948739528656006\n",
            "tensor(0.1293, grad_fn=<MseLossBackward0>)\n",
            "epoch 599, loss 0.12933364510536194\n",
            "tensor(0.1292, grad_fn=<MseLossBackward0>)\n",
            "epoch 600, loss 0.12918098270893097\n",
            "tensor(0.1290, grad_fn=<MseLossBackward0>)\n",
            "epoch 601, loss 0.12902939319610596\n",
            "tensor(0.1289, grad_fn=<MseLossBackward0>)\n",
            "epoch 602, loss 0.12887883186340332\n",
            "tensor(0.1287, grad_fn=<MseLossBackward0>)\n",
            "epoch 603, loss 0.12872928380966187\n",
            "tensor(0.1286, grad_fn=<MseLossBackward0>)\n",
            "epoch 604, loss 0.12858079373836517\n",
            "tensor(0.1284, grad_fn=<MseLossBackward0>)\n",
            "epoch 605, loss 0.12843333184719086\n",
            "tensor(0.1283, grad_fn=<MseLossBackward0>)\n",
            "epoch 606, loss 0.12828686833381653\n",
            "tensor(0.1281, grad_fn=<MseLossBackward0>)\n",
            "epoch 607, loss 0.12814141809940338\n",
            "tensor(0.1280, grad_fn=<MseLossBackward0>)\n",
            "epoch 608, loss 0.12799696624279022\n",
            "tensor(0.1279, grad_fn=<MseLossBackward0>)\n",
            "epoch 609, loss 0.12785351276397705\n",
            "tensor(0.1277, grad_fn=<MseLossBackward0>)\n",
            "epoch 610, loss 0.12771104276180267\n",
            "tensor(0.1276, grad_fn=<MseLossBackward0>)\n",
            "epoch 611, loss 0.12756957113742828\n",
            "tensor(0.1274, grad_fn=<MseLossBackward0>)\n",
            "epoch 612, loss 0.1274290829896927\n",
            "tensor(0.1273, grad_fn=<MseLossBackward0>)\n",
            "epoch 613, loss 0.1272895634174347\n",
            "tensor(0.1272, grad_fn=<MseLossBackward0>)\n",
            "epoch 614, loss 0.12715096771717072\n",
            "tensor(0.1270, grad_fn=<MseLossBackward0>)\n",
            "epoch 615, loss 0.12701332569122314\n",
            "tensor(0.1269, grad_fn=<MseLossBackward0>)\n",
            "epoch 616, loss 0.12687665224075317\n",
            "tensor(0.1267, grad_fn=<MseLossBackward0>)\n",
            "epoch 617, loss 0.12674091756343842\n",
            "tensor(0.1266, grad_fn=<MseLossBackward0>)\n",
            "epoch 618, loss 0.12660612165927887\n",
            "tensor(0.1265, grad_fn=<MseLossBackward0>)\n",
            "epoch 619, loss 0.12647224962711334\n",
            "tensor(0.1263, grad_fn=<MseLossBackward0>)\n",
            "epoch 620, loss 0.12633931636810303\n",
            "tensor(0.1262, grad_fn=<MseLossBackward0>)\n",
            "epoch 621, loss 0.12620724737644196\n",
            "tensor(0.1261, grad_fn=<MseLossBackward0>)\n",
            "epoch 622, loss 0.1260761320590973\n",
            "tensor(0.1259, grad_fn=<MseLossBackward0>)\n",
            "epoch 623, loss 0.12594591081142426\n",
            "tensor(0.1258, grad_fn=<MseLossBackward0>)\n",
            "epoch 624, loss 0.12581659853458405\n",
            "tensor(0.1257, grad_fn=<MseLossBackward0>)\n",
            "epoch 625, loss 0.12568815052509308\n",
            "tensor(0.1256, grad_fn=<MseLossBackward0>)\n",
            "epoch 626, loss 0.12556061148643494\n",
            "tensor(0.1254, grad_fn=<MseLossBackward0>)\n",
            "epoch 627, loss 0.12543395161628723\n",
            "tensor(0.1253, grad_fn=<MseLossBackward0>)\n",
            "epoch 628, loss 0.12530812621116638\n",
            "tensor(0.1252, grad_fn=<MseLossBackward0>)\n",
            "epoch 629, loss 0.12518317997455597\n",
            "tensor(0.1251, grad_fn=<MseLossBackward0>)\n",
            "epoch 630, loss 0.1250590980052948\n",
            "tensor(0.1249, grad_fn=<MseLossBackward0>)\n",
            "epoch 631, loss 0.12493588030338287\n",
            "tensor(0.1248, grad_fn=<MseLossBackward0>)\n",
            "epoch 632, loss 0.124813511967659\n",
            "tensor(0.1247, grad_fn=<MseLossBackward0>)\n",
            "epoch 633, loss 0.12469194829463959\n",
            "tensor(0.1246, grad_fn=<MseLossBackward0>)\n",
            "epoch 634, loss 0.12457126379013062\n",
            "tensor(0.1245, grad_fn=<MseLossBackward0>)\n",
            "epoch 635, loss 0.1244514137506485\n",
            "tensor(0.1243, grad_fn=<MseLossBackward0>)\n",
            "epoch 636, loss 0.12433237582445145\n",
            "tensor(0.1242, grad_fn=<MseLossBackward0>)\n",
            "epoch 637, loss 0.12421415746212006\n",
            "tensor(0.1241, grad_fn=<MseLossBackward0>)\n",
            "epoch 638, loss 0.12409678101539612\n",
            "tensor(0.1240, grad_fn=<MseLossBackward0>)\n",
            "epoch 639, loss 0.12398019433021545\n",
            "tensor(0.1239, grad_fn=<MseLossBackward0>)\n",
            "epoch 640, loss 0.12386441230773926\n",
            "tensor(0.1237, grad_fn=<MseLossBackward0>)\n",
            "epoch 641, loss 0.12374940514564514\n",
            "tensor(0.1236, grad_fn=<MseLossBackward0>)\n",
            "epoch 642, loss 0.12363521009683609\n",
            "tensor(0.1235, grad_fn=<MseLossBackward0>)\n",
            "epoch 643, loss 0.12352180480957031\n",
            "tensor(0.1234, grad_fn=<MseLossBackward0>)\n",
            "epoch 644, loss 0.12340918183326721\n",
            "tensor(0.1233, grad_fn=<MseLossBackward0>)\n",
            "epoch 645, loss 0.1232973262667656\n",
            "tensor(0.1232, grad_fn=<MseLossBackward0>)\n",
            "epoch 646, loss 0.12318621575832367\n",
            "tensor(0.1231, grad_fn=<MseLossBackward0>)\n",
            "epoch 647, loss 0.12307591736316681\n",
            "tensor(0.1230, grad_fn=<MseLossBackward0>)\n",
            "epoch 648, loss 0.12296631932258606\n",
            "tensor(0.1229, grad_fn=<MseLossBackward0>)\n",
            "epoch 649, loss 0.12285749614238739\n",
            "tensor(0.1227, grad_fn=<MseLossBackward0>)\n",
            "epoch 650, loss 0.1227494329214096\n",
            "tensor(0.1226, grad_fn=<MseLossBackward0>)\n",
            "epoch 651, loss 0.12264212220907211\n",
            "tensor(0.1225, grad_fn=<MseLossBackward0>)\n",
            "epoch 652, loss 0.12253556400537491\n",
            "tensor(0.1224, grad_fn=<MseLossBackward0>)\n",
            "epoch 653, loss 0.12242970615625381\n",
            "tensor(0.1223, grad_fn=<MseLossBackward0>)\n",
            "epoch 654, loss 0.1223246157169342\n",
            "tensor(0.1222, grad_fn=<MseLossBackward0>)\n",
            "epoch 655, loss 0.1222202256321907\n",
            "tensor(0.1221, grad_fn=<MseLossBackward0>)\n",
            "epoch 656, loss 0.1221165582537651\n",
            "tensor(0.1220, grad_fn=<MseLossBackward0>)\n",
            "epoch 657, loss 0.12201361358165741\n",
            "tensor(0.1219, grad_fn=<MseLossBackward0>)\n",
            "epoch 658, loss 0.12191137671470642\n",
            "tensor(0.1218, grad_fn=<MseLossBackward0>)\n",
            "epoch 659, loss 0.12180985510349274\n",
            "tensor(0.1217, grad_fn=<MseLossBackward0>)\n",
            "epoch 660, loss 0.12170898914337158\n",
            "tensor(0.1216, grad_fn=<MseLossBackward0>)\n",
            "epoch 661, loss 0.12160885334014893\n",
            "tensor(0.1215, grad_fn=<MseLossBackward0>)\n",
            "epoch 662, loss 0.1215093806385994\n",
            "tensor(0.1214, grad_fn=<MseLossBackward0>)\n",
            "epoch 663, loss 0.12141061574220657\n",
            "tensor(0.1213, grad_fn=<MseLossBackward0>)\n",
            "epoch 664, loss 0.12131252139806747\n",
            "tensor(0.1212, grad_fn=<MseLossBackward0>)\n",
            "epoch 665, loss 0.12121512740850449\n",
            "tensor(0.1211, grad_fn=<MseLossBackward0>)\n",
            "epoch 666, loss 0.12111838907003403\n",
            "tensor(0.1210, grad_fn=<MseLossBackward0>)\n",
            "epoch 667, loss 0.1210223138332367\n",
            "tensor(0.1209, grad_fn=<MseLossBackward0>)\n",
            "epoch 668, loss 0.12092690914869308\n",
            "tensor(0.1208, grad_fn=<MseLossBackward0>)\n",
            "epoch 669, loss 0.12083214521408081\n",
            "tensor(0.1207, grad_fn=<MseLossBackward0>)\n",
            "epoch 670, loss 0.12073805928230286\n",
            "tensor(0.1206, grad_fn=<MseLossBackward0>)\n",
            "epoch 671, loss 0.12064460664987564\n",
            "tensor(0.1206, grad_fn=<MseLossBackward0>)\n",
            "epoch 672, loss 0.12055177241563797\n",
            "tensor(0.1205, grad_fn=<MseLossBackward0>)\n",
            "epoch 673, loss 0.12045958638191223\n",
            "tensor(0.1204, grad_fn=<MseLossBackward0>)\n",
            "epoch 674, loss 0.12036805599927902\n",
            "tensor(0.1203, grad_fn=<MseLossBackward0>)\n",
            "epoch 675, loss 0.12027715891599655\n",
            "tensor(0.1202, grad_fn=<MseLossBackward0>)\n",
            "epoch 676, loss 0.12018685787916183\n",
            "tensor(0.1201, grad_fn=<MseLossBackward0>)\n",
            "epoch 677, loss 0.12009721249341965\n",
            "tensor(0.1200, grad_fn=<MseLossBackward0>)\n",
            "epoch 678, loss 0.12000817060470581\n",
            "tensor(0.1199, grad_fn=<MseLossBackward0>)\n",
            "epoch 679, loss 0.11991973966360092\n",
            "tensor(0.1198, grad_fn=<MseLossBackward0>)\n",
            "epoch 680, loss 0.11983190476894379\n",
            "tensor(0.1197, grad_fn=<MseLossBackward0>)\n",
            "epoch 681, loss 0.11974470317363739\n",
            "tensor(0.1197, grad_fn=<MseLossBackward0>)\n",
            "epoch 682, loss 0.11965807527303696\n",
            "tensor(0.1196, grad_fn=<MseLossBackward0>)\n",
            "epoch 683, loss 0.11957206577062607\n",
            "tensor(0.1195, grad_fn=<MseLossBackward0>)\n",
            "epoch 684, loss 0.11948663741350174\n",
            "tensor(0.1194, grad_fn=<MseLossBackward0>)\n",
            "epoch 685, loss 0.11940179020166397\n",
            "tensor(0.1193, grad_fn=<MseLossBackward0>)\n",
            "epoch 686, loss 0.11931753903627396\n",
            "tensor(0.1192, grad_fn=<MseLossBackward0>)\n",
            "epoch 687, loss 0.1192338764667511\n",
            "tensor(0.1192, grad_fn=<MseLossBackward0>)\n",
            "epoch 688, loss 0.11915076524019241\n",
            "tensor(0.1191, grad_fn=<MseLossBackward0>)\n",
            "epoch 689, loss 0.11906825006008148\n",
            "tensor(0.1190, grad_fn=<MseLossBackward0>)\n",
            "epoch 690, loss 0.11898628622293472\n",
            "tensor(0.1189, grad_fn=<MseLossBackward0>)\n",
            "epoch 691, loss 0.11890491098165512\n",
            "tensor(0.1188, grad_fn=<MseLossBackward0>)\n",
            "epoch 692, loss 0.11882409453392029\n",
            "tensor(0.1187, grad_fn=<MseLossBackward0>)\n",
            "epoch 693, loss 0.11874382942914963\n",
            "tensor(0.1187, grad_fn=<MseLossBackward0>)\n",
            "epoch 694, loss 0.11866408586502075\n",
            "tensor(0.1186, grad_fn=<MseLossBackward0>)\n",
            "epoch 695, loss 0.11858490854501724\n",
            "tensor(0.1185, grad_fn=<MseLossBackward0>)\n",
            "epoch 696, loss 0.1185062900185585\n",
            "tensor(0.1184, grad_fn=<MseLossBackward0>)\n",
            "epoch 697, loss 0.11842820048332214\n",
            "tensor(0.1184, grad_fn=<MseLossBackward0>)\n",
            "epoch 698, loss 0.11835063993930817\n",
            "tensor(0.1183, grad_fn=<MseLossBackward0>)\n",
            "epoch 699, loss 0.11827364563941956\n",
            "tensor(0.1182, grad_fn=<MseLossBackward0>)\n",
            "epoch 700, loss 0.11819716542959213\n",
            "tensor(0.1181, grad_fn=<MseLossBackward0>)\n",
            "epoch 701, loss 0.11812121421098709\n",
            "tensor(0.1180, grad_fn=<MseLossBackward0>)\n",
            "epoch 702, loss 0.11804579198360443\n",
            "tensor(0.1180, grad_fn=<MseLossBackward0>)\n",
            "epoch 703, loss 0.11797086894512177\n",
            "tensor(0.1179, grad_fn=<MseLossBackward0>)\n",
            "epoch 704, loss 0.11789645999670029\n",
            "tensor(0.1178, grad_fn=<MseLossBackward0>)\n",
            "epoch 705, loss 0.11782258003950119\n",
            "tensor(0.1177, grad_fn=<MseLossBackward0>)\n",
            "epoch 706, loss 0.11774919927120209\n",
            "tensor(0.1177, grad_fn=<MseLossBackward0>)\n",
            "epoch 707, loss 0.11767633259296417\n",
            "tensor(0.1176, grad_fn=<MseLossBackward0>)\n",
            "epoch 708, loss 0.11760396510362625\n",
            "tensor(0.1175, grad_fn=<MseLossBackward0>)\n",
            "epoch 709, loss 0.11753208935260773\n",
            "tensor(0.1175, grad_fn=<MseLossBackward0>)\n",
            "epoch 710, loss 0.1174607053399086\n",
            "tensor(0.1174, grad_fn=<MseLossBackward0>)\n",
            "epoch 711, loss 0.11738983541727066\n",
            "tensor(0.1173, grad_fn=<MseLossBackward0>)\n",
            "epoch 712, loss 0.11731943488121033\n",
            "tensor(0.1172, grad_fn=<MseLossBackward0>)\n",
            "epoch 713, loss 0.11724952608346939\n",
            "tensor(0.1172, grad_fn=<MseLossBackward0>)\n",
            "epoch 714, loss 0.11718010902404785\n",
            "tensor(0.1171, grad_fn=<MseLossBackward0>)\n",
            "epoch 715, loss 0.11711113899946213\n",
            "tensor(0.1170, grad_fn=<MseLossBackward0>)\n",
            "epoch 716, loss 0.117042675614357\n",
            "tensor(0.1170, grad_fn=<MseLossBackward0>)\n",
            "epoch 717, loss 0.11697465926408768\n",
            "tensor(0.1169, grad_fn=<MseLossBackward0>)\n",
            "epoch 718, loss 0.11690713465213776\n",
            "tensor(0.1168, grad_fn=<MseLossBackward0>)\n",
            "epoch 719, loss 0.11684006452560425\n",
            "tensor(0.1168, grad_fn=<MseLossBackward0>)\n",
            "epoch 720, loss 0.11677344888448715\n",
            "tensor(0.1167, grad_fn=<MseLossBackward0>)\n",
            "epoch 721, loss 0.11670729517936707\n",
            "tensor(0.1166, grad_fn=<MseLossBackward0>)\n",
            "epoch 722, loss 0.11664160341024399\n",
            "tensor(0.1166, grad_fn=<MseLossBackward0>)\n",
            "epoch 723, loss 0.11657635122537613\n",
            "tensor(0.1165, grad_fn=<MseLossBackward0>)\n",
            "epoch 724, loss 0.11651156842708588\n",
            "tensor(0.1164, grad_fn=<MseLossBackward0>)\n",
            "epoch 725, loss 0.11644721031188965\n",
            "tensor(0.1164, grad_fn=<MseLossBackward0>)\n",
            "epoch 726, loss 0.11638331413269043\n",
            "tensor(0.1163, grad_fn=<MseLossBackward0>)\n",
            "epoch 727, loss 0.11631985008716583\n",
            "tensor(0.1163, grad_fn=<MseLossBackward0>)\n",
            "epoch 728, loss 0.11625683307647705\n",
            "tensor(0.1162, grad_fn=<MseLossBackward0>)\n",
            "epoch 729, loss 0.1161942407488823\n",
            "tensor(0.1161, grad_fn=<MseLossBackward0>)\n",
            "epoch 730, loss 0.11613205820322037\n",
            "tensor(0.1161, grad_fn=<MseLossBackward0>)\n",
            "epoch 731, loss 0.11607034504413605\n",
            "tensor(0.1160, grad_fn=<MseLossBackward0>)\n",
            "epoch 732, loss 0.11600904911756516\n",
            "tensor(0.1159, grad_fn=<MseLossBackward0>)\n",
            "epoch 733, loss 0.1159481555223465\n",
            "tensor(0.1159, grad_fn=<MseLossBackward0>)\n",
            "epoch 734, loss 0.11588767915964127\n",
            "tensor(0.1158, grad_fn=<MseLossBackward0>)\n",
            "epoch 735, loss 0.11582762748003006\n",
            "tensor(0.1158, grad_fn=<MseLossBackward0>)\n",
            "epoch 736, loss 0.11576797813177109\n",
            "tensor(0.1157, grad_fn=<MseLossBackward0>)\n",
            "epoch 737, loss 0.11570874601602554\n",
            "tensor(0.1156, grad_fn=<MseLossBackward0>)\n",
            "epoch 738, loss 0.11564993858337402\n",
            "tensor(0.1156, grad_fn=<MseLossBackward0>)\n",
            "epoch 739, loss 0.11559151858091354\n",
            "tensor(0.1155, grad_fn=<MseLossBackward0>)\n",
            "epoch 740, loss 0.1155335009098053\n",
            "tensor(0.1155, grad_fn=<MseLossBackward0>)\n",
            "epoch 741, loss 0.11547587811946869\n",
            "tensor(0.1154, grad_fn=<MseLossBackward0>)\n",
            "epoch 742, loss 0.11541865766048431\n",
            "tensor(0.1154, grad_fn=<MseLossBackward0>)\n",
            "epoch 743, loss 0.11536182463169098\n",
            "tensor(0.1153, grad_fn=<MseLossBackward0>)\n",
            "epoch 744, loss 0.11530540138483047\n",
            "tensor(0.1152, grad_fn=<MseLossBackward0>)\n",
            "epoch 745, loss 0.11524937301874161\n",
            "tensor(0.1152, grad_fn=<MseLossBackward0>)\n",
            "epoch 746, loss 0.11519370973110199\n",
            "tensor(0.1151, grad_fn=<MseLossBackward0>)\n",
            "epoch 747, loss 0.1151384487748146\n",
            "tensor(0.1151, grad_fn=<MseLossBackward0>)\n",
            "epoch 748, loss 0.11508356034755707\n",
            "tensor(0.1150, grad_fn=<MseLossBackward0>)\n",
            "epoch 749, loss 0.11502903699874878\n",
            "tensor(0.1150, grad_fn=<MseLossBackward0>)\n",
            "epoch 750, loss 0.11497489362955093\n",
            "tensor(0.1149, grad_fn=<MseLossBackward0>)\n",
            "epoch 751, loss 0.11492113769054413\n",
            "tensor(0.1149, grad_fn=<MseLossBackward0>)\n",
            "epoch 752, loss 0.11486773192882538\n",
            "tensor(0.1148, grad_fn=<MseLossBackward0>)\n",
            "epoch 753, loss 0.11481470614671707\n",
            "tensor(0.1148, grad_fn=<MseLossBackward0>)\n",
            "epoch 754, loss 0.11476204544305801\n",
            "tensor(0.1147, grad_fn=<MseLossBackward0>)\n",
            "epoch 755, loss 0.11470973491668701\n",
            "tensor(0.1147, grad_fn=<MseLossBackward0>)\n",
            "epoch 756, loss 0.11465781927108765\n",
            "tensor(0.1146, grad_fn=<MseLossBackward0>)\n",
            "epoch 757, loss 0.11460623145103455\n",
            "tensor(0.1146, grad_fn=<MseLossBackward0>)\n",
            "epoch 758, loss 0.1145549938082695\n",
            "tensor(0.1145, grad_fn=<MseLossBackward0>)\n",
            "epoch 759, loss 0.1145041212439537\n",
            "tensor(0.1145, grad_fn=<MseLossBackward0>)\n",
            "epoch 760, loss 0.11445361375808716\n",
            "tensor(0.1144, grad_fn=<MseLossBackward0>)\n",
            "epoch 761, loss 0.11440343409776688\n",
            "tensor(0.1144, grad_fn=<MseLossBackward0>)\n",
            "epoch 762, loss 0.11435361206531525\n",
            "tensor(0.1143, grad_fn=<MseLossBackward0>)\n",
            "epoch 763, loss 0.11430412530899048\n",
            "tensor(0.1143, grad_fn=<MseLossBackward0>)\n",
            "epoch 764, loss 0.11425496637821198\n",
            "tensor(0.1142, grad_fn=<MseLossBackward0>)\n",
            "epoch 765, loss 0.11420617252588272\n",
            "tensor(0.1142, grad_fn=<MseLossBackward0>)\n",
            "epoch 766, loss 0.11415767669677734\n",
            "tensor(0.1141, grad_fn=<MseLossBackward0>)\n",
            "epoch 767, loss 0.11410954594612122\n",
            "tensor(0.1141, grad_fn=<MseLossBackward0>)\n",
            "epoch 768, loss 0.11406173557043076\n",
            "tensor(0.1140, grad_fn=<MseLossBackward0>)\n",
            "epoch 769, loss 0.11401426047086716\n",
            "tensor(0.1140, grad_fn=<MseLossBackward0>)\n",
            "epoch 770, loss 0.11396711319684982\n",
            "tensor(0.1139, grad_fn=<MseLossBackward0>)\n",
            "epoch 771, loss 0.11392029374837875\n",
            "tensor(0.1139, grad_fn=<MseLossBackward0>)\n",
            "epoch 772, loss 0.11387377977371216\n",
            "tensor(0.1138, grad_fn=<MseLossBackward0>)\n",
            "epoch 773, loss 0.11382761597633362\n",
            "tensor(0.1138, grad_fn=<MseLossBackward0>)\n",
            "epoch 774, loss 0.11378175765275955\n",
            "tensor(0.1137, grad_fn=<MseLossBackward0>)\n",
            "epoch 775, loss 0.11373620480298996\n",
            "tensor(0.1137, grad_fn=<MseLossBackward0>)\n",
            "epoch 776, loss 0.11369097977876663\n",
            "tensor(0.1136, grad_fn=<MseLossBackward0>)\n",
            "epoch 777, loss 0.11364606022834778\n",
            "tensor(0.1136, grad_fn=<MseLossBackward0>)\n",
            "epoch 778, loss 0.11360142379999161\n",
            "tensor(0.1136, grad_fn=<MseLossBackward0>)\n",
            "epoch 779, loss 0.11355713754892349\n",
            "tensor(0.1135, grad_fn=<MseLossBackward0>)\n",
            "epoch 780, loss 0.11351313441991806\n",
            "tensor(0.1135, grad_fn=<MseLossBackward0>)\n",
            "epoch 781, loss 0.11346942186355591\n",
            "tensor(0.1134, grad_fn=<MseLossBackward0>)\n",
            "epoch 782, loss 0.11342605203390121\n",
            "tensor(0.1134, grad_fn=<MseLossBackward0>)\n",
            "epoch 783, loss 0.11338294297456741\n",
            "tensor(0.1133, grad_fn=<MseLossBackward0>)\n",
            "epoch 784, loss 0.11334013938903809\n",
            "tensor(0.1133, grad_fn=<MseLossBackward0>)\n",
            "epoch 785, loss 0.11329764872789383\n",
            "tensor(0.1133, grad_fn=<MseLossBackward0>)\n",
            "epoch 786, loss 0.11325541883707047\n",
            "tensor(0.1132, grad_fn=<MseLossBackward0>)\n",
            "epoch 787, loss 0.11321350187063217\n",
            "tensor(0.1132, grad_fn=<MseLossBackward0>)\n",
            "epoch 788, loss 0.11317187547683716\n",
            "tensor(0.1131, grad_fn=<MseLossBackward0>)\n",
            "epoch 789, loss 0.11313053220510483\n",
            "tensor(0.1131, grad_fn=<MseLossBackward0>)\n",
            "epoch 790, loss 0.11308944225311279\n",
            "tensor(0.1130, grad_fn=<MseLossBackward0>)\n",
            "epoch 791, loss 0.11304868757724762\n",
            "tensor(0.1130, grad_fn=<MseLossBackward0>)\n",
            "epoch 792, loss 0.11300818622112274\n",
            "tensor(0.1130, grad_fn=<MseLossBackward0>)\n",
            "epoch 793, loss 0.11296796053647995\n",
            "tensor(0.1129, grad_fn=<MseLossBackward0>)\n",
            "epoch 794, loss 0.11292804032564163\n",
            "tensor(0.1129, grad_fn=<MseLossBackward0>)\n",
            "epoch 795, loss 0.11288836598396301\n",
            "tensor(0.1128, grad_fn=<MseLossBackward0>)\n",
            "epoch 796, loss 0.11284895986318588\n",
            "tensor(0.1128, grad_fn=<MseLossBackward0>)\n",
            "epoch 797, loss 0.11280983686447144\n",
            "tensor(0.1128, grad_fn=<MseLossBackward0>)\n",
            "epoch 798, loss 0.11277099698781967\n",
            "tensor(0.1127, grad_fn=<MseLossBackward0>)\n",
            "epoch 799, loss 0.1127324029803276\n",
            "tensor(0.1127, grad_fn=<MseLossBackward0>)\n",
            "epoch 800, loss 0.11269407719373703\n",
            "tensor(0.1127, grad_fn=<MseLossBackward0>)\n",
            "epoch 801, loss 0.11265603452920914\n",
            "tensor(0.1126, grad_fn=<MseLossBackward0>)\n",
            "epoch 802, loss 0.11261825263500214\n",
            "tensor(0.1126, grad_fn=<MseLossBackward0>)\n",
            "epoch 803, loss 0.11258070170879364\n",
            "tensor(0.1125, grad_fn=<MseLossBackward0>)\n",
            "epoch 804, loss 0.11254343390464783\n",
            "tensor(0.1125, grad_fn=<MseLossBackward0>)\n",
            "epoch 805, loss 0.11250641942024231\n",
            "tensor(0.1125, grad_fn=<MseLossBackward0>)\n",
            "epoch 806, loss 0.11246965825557709\n",
            "tensor(0.1124, grad_fn=<MseLossBackward0>)\n",
            "epoch 807, loss 0.11243314296007156\n",
            "tensor(0.1124, grad_fn=<MseLossBackward0>)\n",
            "epoch 808, loss 0.11239689588546753\n",
            "tensor(0.1124, grad_fn=<MseLossBackward0>)\n",
            "epoch 809, loss 0.1123608872294426\n",
            "tensor(0.1123, grad_fn=<MseLossBackward0>)\n",
            "epoch 810, loss 0.11232511699199677\n",
            "tensor(0.1123, grad_fn=<MseLossBackward0>)\n",
            "epoch 811, loss 0.11228961497545242\n",
            "tensor(0.1123, grad_fn=<MseLossBackward0>)\n",
            "epoch 812, loss 0.11225433647632599\n",
            "tensor(0.1122, grad_fn=<MseLossBackward0>)\n",
            "epoch 813, loss 0.11221931129693985\n",
            "tensor(0.1122, grad_fn=<MseLossBackward0>)\n",
            "epoch 814, loss 0.11218452453613281\n",
            "tensor(0.1121, grad_fn=<MseLossBackward0>)\n",
            "epoch 815, loss 0.11214998364448547\n",
            "tensor(0.1121, grad_fn=<MseLossBackward0>)\n",
            "epoch 816, loss 0.11211566627025604\n",
            "tensor(0.1121, grad_fn=<MseLossBackward0>)\n",
            "epoch 817, loss 0.1120816171169281\n",
            "tensor(0.1120, grad_fn=<MseLossBackward0>)\n",
            "epoch 818, loss 0.11204776912927628\n",
            "tensor(0.1120, grad_fn=<MseLossBackward0>)\n",
            "epoch 819, loss 0.11201415956020355\n",
            "tensor(0.1120, grad_fn=<MseLossBackward0>)\n",
            "epoch 820, loss 0.11198078840970993\n",
            "tensor(0.1119, grad_fn=<MseLossBackward0>)\n",
            "epoch 821, loss 0.11194764822721481\n",
            "tensor(0.1119, grad_fn=<MseLossBackward0>)\n",
            "epoch 822, loss 0.1119147390127182\n",
            "tensor(0.1119, grad_fn=<MseLossBackward0>)\n",
            "epoch 823, loss 0.1118820384144783\n",
            "tensor(0.1118, grad_fn=<MseLossBackward0>)\n",
            "epoch 824, loss 0.11184956133365631\n",
            "tensor(0.1118, grad_fn=<MseLossBackward0>)\n",
            "epoch 825, loss 0.11181734502315521\n",
            "tensor(0.1118, grad_fn=<MseLossBackward0>)\n",
            "epoch 826, loss 0.11178532987833023\n",
            "tensor(0.1118, grad_fn=<MseLossBackward0>)\n",
            "epoch 827, loss 0.11175353080034256\n",
            "tensor(0.1117, grad_fn=<MseLossBackward0>)\n",
            "epoch 828, loss 0.1117219626903534\n",
            "tensor(0.1117, grad_fn=<MseLossBackward0>)\n",
            "epoch 829, loss 0.11169060319662094\n",
            "tensor(0.1117, grad_fn=<MseLossBackward0>)\n",
            "epoch 830, loss 0.1116594523191452\n",
            "tensor(0.1116, grad_fn=<MseLossBackward0>)\n",
            "epoch 831, loss 0.11162851750850677\n",
            "tensor(0.1116, grad_fn=<MseLossBackward0>)\n",
            "epoch 832, loss 0.11159779131412506\n",
            "tensor(0.1116, grad_fn=<MseLossBackward0>)\n",
            "epoch 833, loss 0.11156730353832245\n",
            "tensor(0.1115, grad_fn=<MseLossBackward0>)\n",
            "epoch 834, loss 0.11153700947761536\n",
            "tensor(0.1115, grad_fn=<MseLossBackward0>)\n",
            "epoch 835, loss 0.11150691658258438\n",
            "tensor(0.1115, grad_fn=<MseLossBackward0>)\n",
            "epoch 836, loss 0.11147703975439072\n",
            "tensor(0.1114, grad_fn=<MseLossBackward0>)\n",
            "epoch 837, loss 0.11144737154245377\n",
            "tensor(0.1114, grad_fn=<MseLossBackward0>)\n",
            "epoch 838, loss 0.11141790449619293\n",
            "tensor(0.1114, grad_fn=<MseLossBackward0>)\n",
            "epoch 839, loss 0.11138863861560822\n",
            "tensor(0.1114, grad_fn=<MseLossBackward0>)\n",
            "epoch 840, loss 0.11135956645011902\n",
            "tensor(0.1113, grad_fn=<MseLossBackward0>)\n",
            "epoch 841, loss 0.11133069545030594\n",
            "tensor(0.1113, grad_fn=<MseLossBackward0>)\n",
            "epoch 842, loss 0.11130203306674957\n",
            "tensor(0.1113, grad_fn=<MseLossBackward0>)\n",
            "epoch 843, loss 0.11127357184886932\n",
            "tensor(0.1112, grad_fn=<MseLossBackward0>)\n",
            "epoch 844, loss 0.111245296895504\n",
            "tensor(0.1112, grad_fn=<MseLossBackward0>)\n",
            "epoch 845, loss 0.1112172082066536\n",
            "tensor(0.1112, grad_fn=<MseLossBackward0>)\n",
            "epoch 846, loss 0.1111893355846405\n",
            "tensor(0.1112, grad_fn=<MseLossBackward0>)\n",
            "epoch 847, loss 0.11116164922714233\n",
            "tensor(0.1111, grad_fn=<MseLossBackward0>)\n",
            "epoch 848, loss 0.11113414913415909\n",
            "tensor(0.1111, grad_fn=<MseLossBackward0>)\n",
            "epoch 849, loss 0.11110680550336838\n",
            "tensor(0.1111, grad_fn=<MseLossBackward0>)\n",
            "epoch 850, loss 0.11107970029115677\n",
            "tensor(0.1111, grad_fn=<MseLossBackward0>)\n",
            "epoch 851, loss 0.1110527515411377\n",
            "tensor(0.1110, grad_fn=<MseLossBackward0>)\n",
            "epoch 852, loss 0.11102600395679474\n",
            "tensor(0.1110, grad_fn=<MseLossBackward0>)\n",
            "epoch 853, loss 0.11099943518638611\n",
            "tensor(0.1110, grad_fn=<MseLossBackward0>)\n",
            "epoch 854, loss 0.1109730675816536\n",
            "tensor(0.1109, grad_fn=<MseLossBackward0>)\n",
            "epoch 855, loss 0.11094686388969421\n",
            "tensor(0.1109, grad_fn=<MseLossBackward0>)\n",
            "epoch 856, loss 0.11092082411050797\n",
            "tensor(0.1109, grad_fn=<MseLossBackward0>)\n",
            "epoch 857, loss 0.11089500039815903\n",
            "tensor(0.1109, grad_fn=<MseLossBackward0>)\n",
            "epoch 858, loss 0.11086934059858322\n",
            "tensor(0.1108, grad_fn=<MseLossBackward0>)\n",
            "epoch 859, loss 0.11084385216236115\n",
            "tensor(0.1108, grad_fn=<MseLossBackward0>)\n",
            "epoch 860, loss 0.1108185350894928\n",
            "tensor(0.1108, grad_fn=<MseLossBackward0>)\n",
            "epoch 861, loss 0.11079338937997818\n",
            "tensor(0.1108, grad_fn=<MseLossBackward0>)\n",
            "epoch 862, loss 0.11076845228672028\n",
            "tensor(0.1107, grad_fn=<MseLossBackward0>)\n",
            "epoch 863, loss 0.11074365675449371\n",
            "tensor(0.1107, grad_fn=<MseLossBackward0>)\n",
            "epoch 864, loss 0.11071901768445969\n",
            "tensor(0.1107, grad_fn=<MseLossBackward0>)\n",
            "epoch 865, loss 0.11069457232952118\n",
            "tensor(0.1107, grad_fn=<MseLossBackward0>)\n",
            "epoch 866, loss 0.11067028343677521\n",
            "tensor(0.1106, grad_fn=<MseLossBackward0>)\n",
            "epoch 867, loss 0.11064616590738297\n",
            "tensor(0.1106, grad_fn=<MseLossBackward0>)\n",
            "epoch 868, loss 0.11062221229076385\n",
            "tensor(0.1106, grad_fn=<MseLossBackward0>)\n",
            "epoch 869, loss 0.11059842258691788\n",
            "tensor(0.1106, grad_fn=<MseLossBackward0>)\n",
            "epoch 870, loss 0.11057481169700623\n",
            "tensor(0.1106, grad_fn=<MseLossBackward0>)\n",
            "epoch 871, loss 0.11055133491754532\n",
            "tensor(0.1105, grad_fn=<MseLossBackward0>)\n",
            "epoch 872, loss 0.11052804440259933\n",
            "tensor(0.1105, grad_fn=<MseLossBackward0>)\n",
            "epoch 873, loss 0.1105048805475235\n",
            "tensor(0.1105, grad_fn=<MseLossBackward0>)\n",
            "epoch 874, loss 0.11048191785812378\n",
            "tensor(0.1105, grad_fn=<MseLossBackward0>)\n",
            "epoch 875, loss 0.110459104180336\n",
            "tensor(0.1104, grad_fn=<MseLossBackward0>)\n",
            "epoch 876, loss 0.11043645441532135\n",
            "tensor(0.1104, grad_fn=<MseLossBackward0>)\n",
            "epoch 877, loss 0.11041394621133804\n",
            "tensor(0.1104, grad_fn=<MseLossBackward0>)\n",
            "epoch 878, loss 0.11039158701896667\n",
            "tensor(0.1104, grad_fn=<MseLossBackward0>)\n",
            "epoch 879, loss 0.11036938428878784\n",
            "tensor(0.1103, grad_fn=<MseLossBackward0>)\n",
            "epoch 880, loss 0.11034734547138214\n",
            "tensor(0.1103, grad_fn=<MseLossBackward0>)\n",
            "epoch 881, loss 0.11032544821500778\n",
            "tensor(0.1103, grad_fn=<MseLossBackward0>)\n",
            "epoch 882, loss 0.11030372232198715\n",
            "tensor(0.1103, grad_fn=<MseLossBackward0>)\n",
            "epoch 883, loss 0.11028211563825607\n",
            "tensor(0.1103, grad_fn=<MseLossBackward0>)\n",
            "epoch 884, loss 0.11026067286729813\n",
            "tensor(0.1102, grad_fn=<MseLossBackward0>)\n",
            "epoch 885, loss 0.11023939400911331\n",
            "tensor(0.1102, grad_fn=<MseLossBackward0>)\n",
            "epoch 886, loss 0.11021821945905685\n",
            "tensor(0.1102, grad_fn=<MseLossBackward0>)\n",
            "epoch 887, loss 0.11019722372293472\n",
            "tensor(0.1102, grad_fn=<MseLossBackward0>)\n",
            "epoch 888, loss 0.11017637699842453\n",
            "tensor(0.1102, grad_fn=<MseLossBackward0>)\n",
            "epoch 889, loss 0.11015565693378448\n",
            "tensor(0.1101, grad_fn=<MseLossBackward0>)\n",
            "epoch 890, loss 0.11013507843017578\n",
            "tensor(0.1101, grad_fn=<MseLossBackward0>)\n",
            "epoch 891, loss 0.11011464893817902\n",
            "tensor(0.1101, grad_fn=<MseLossBackward0>)\n",
            "epoch 892, loss 0.11009436100721359\n",
            "tensor(0.1101, grad_fn=<MseLossBackward0>)\n",
            "epoch 893, loss 0.11007421463727951\n",
            "tensor(0.1101, grad_fn=<MseLossBackward0>)\n",
            "epoch 894, loss 0.11005420237779617\n",
            "tensor(0.1100, grad_fn=<MseLossBackward0>)\n",
            "epoch 895, loss 0.11003431677818298\n",
            "tensor(0.1100, grad_fn=<MseLossBackward0>)\n",
            "epoch 896, loss 0.11001458764076233\n",
            "tensor(0.1100, grad_fn=<MseLossBackward0>)\n",
            "epoch 897, loss 0.10999497771263123\n",
            "tensor(0.1100, grad_fn=<MseLossBackward0>)\n",
            "epoch 898, loss 0.10997551679611206\n",
            "tensor(0.1100, grad_fn=<MseLossBackward0>)\n",
            "epoch 899, loss 0.10995618253946304\n",
            "tensor(0.1099, grad_fn=<MseLossBackward0>)\n",
            "epoch 900, loss 0.10993697494268417\n",
            "tensor(0.1099, grad_fn=<MseLossBackward0>)\n",
            "epoch 901, loss 0.10991790145635605\n",
            "tensor(0.1099, grad_fn=<MseLossBackward0>)\n",
            "epoch 902, loss 0.10989898443222046\n",
            "tensor(0.1099, grad_fn=<MseLossBackward0>)\n",
            "epoch 903, loss 0.10988017171621323\n",
            "tensor(0.1099, grad_fn=<MseLossBackward0>)\n",
            "epoch 904, loss 0.10986150801181793\n",
            "tensor(0.1098, grad_fn=<MseLossBackward0>)\n",
            "epoch 905, loss 0.10984295606613159\n",
            "tensor(0.1098, grad_fn=<MseLossBackward0>)\n",
            "epoch 906, loss 0.1098245233297348\n",
            "tensor(0.1098, grad_fn=<MseLossBackward0>)\n",
            "epoch 907, loss 0.10980624705553055\n",
            "tensor(0.1098, grad_fn=<MseLossBackward0>)\n",
            "epoch 908, loss 0.10978807508945465\n",
            "tensor(0.1098, grad_fn=<MseLossBackward0>)\n",
            "epoch 909, loss 0.1097700297832489\n",
            "tensor(0.1098, grad_fn=<MseLossBackward0>)\n",
            "epoch 910, loss 0.1097521036863327\n",
            "tensor(0.1097, grad_fn=<MseLossBackward0>)\n",
            "epoch 911, loss 0.10973434150218964\n",
            "tensor(0.1097, grad_fn=<MseLossBackward0>)\n",
            "epoch 912, loss 0.10971665382385254\n",
            "tensor(0.1097, grad_fn=<MseLossBackward0>)\n",
            "epoch 913, loss 0.10969910770654678\n",
            "tensor(0.1097, grad_fn=<MseLossBackward0>)\n",
            "epoch 914, loss 0.10968168079853058\n",
            "tensor(0.1097, grad_fn=<MseLossBackward0>)\n",
            "epoch 915, loss 0.10966438055038452\n",
            "tensor(0.1096, grad_fn=<MseLossBackward0>)\n",
            "epoch 916, loss 0.10964717715978622\n",
            "tensor(0.1096, grad_fn=<MseLossBackward0>)\n",
            "epoch 917, loss 0.10963013023138046\n",
            "tensor(0.1096, grad_fn=<MseLossBackward0>)\n",
            "epoch 918, loss 0.10961315780878067\n",
            "tensor(0.1096, grad_fn=<MseLossBackward0>)\n",
            "epoch 919, loss 0.10959634184837341\n",
            "tensor(0.1096, grad_fn=<MseLossBackward0>)\n",
            "epoch 920, loss 0.10957960784435272\n",
            "tensor(0.1096, grad_fn=<MseLossBackward0>)\n",
            "epoch 921, loss 0.10956301540136337\n",
            "tensor(0.1095, grad_fn=<MseLossBackward0>)\n",
            "epoch 922, loss 0.10954650491476059\n",
            "tensor(0.1095, grad_fn=<MseLossBackward0>)\n",
            "epoch 923, loss 0.10953013598918915\n",
            "tensor(0.1095, grad_fn=<MseLossBackward0>)\n",
            "epoch 924, loss 0.10951387137174606\n",
            "tensor(0.1095, grad_fn=<MseLossBackward0>)\n",
            "epoch 925, loss 0.10949771851301193\n",
            "tensor(0.1095, grad_fn=<MseLossBackward0>)\n",
            "epoch 926, loss 0.10948167741298676\n",
            "tensor(0.1095, grad_fn=<MseLossBackward0>)\n",
            "epoch 927, loss 0.10946574062108994\n",
            "tensor(0.1094, grad_fn=<MseLossBackward0>)\n",
            "epoch 928, loss 0.10944991558790207\n",
            "tensor(0.1094, grad_fn=<MseLossBackward0>)\n",
            "epoch 929, loss 0.10943420976400375\n",
            "tensor(0.1094, grad_fn=<MseLossBackward0>)\n",
            "epoch 930, loss 0.1094186082482338\n",
            "tensor(0.1094, grad_fn=<MseLossBackward0>)\n",
            "epoch 931, loss 0.1094031035900116\n",
            "tensor(0.1094, grad_fn=<MseLossBackward0>)\n",
            "epoch 932, loss 0.10938772559165955\n",
            "tensor(0.1094, grad_fn=<MseLossBackward0>)\n",
            "epoch 933, loss 0.10937243700027466\n",
            "tensor(0.1094, grad_fn=<MseLossBackward0>)\n",
            "epoch 934, loss 0.10935726761817932\n",
            "tensor(0.1093, grad_fn=<MseLossBackward0>)\n",
            "epoch 935, loss 0.10934219509363174\n",
            "tensor(0.1093, grad_fn=<MseLossBackward0>)\n",
            "epoch 936, loss 0.10932719707489014\n",
            "tensor(0.1093, grad_fn=<MseLossBackward0>)\n",
            "epoch 937, loss 0.10931234061717987\n",
            "tensor(0.1093, grad_fn=<MseLossBackward0>)\n",
            "epoch 938, loss 0.10929759591817856\n",
            "tensor(0.1093, grad_fn=<MseLossBackward0>)\n",
            "epoch 939, loss 0.10928291827440262\n",
            "tensor(0.1093, grad_fn=<MseLossBackward0>)\n",
            "epoch 940, loss 0.10926835238933563\n",
            "tensor(0.1093, grad_fn=<MseLossBackward0>)\n",
            "epoch 941, loss 0.109253890812397\n",
            "tensor(0.1092, grad_fn=<MseLossBackward0>)\n",
            "epoch 942, loss 0.10923954844474792\n",
            "tensor(0.1092, grad_fn=<MseLossBackward0>)\n",
            "epoch 943, loss 0.10922527313232422\n",
            "tensor(0.1092, grad_fn=<MseLossBackward0>)\n",
            "epoch 944, loss 0.10921110957860947\n",
            "tensor(0.1092, grad_fn=<MseLossBackward0>)\n",
            "epoch 945, loss 0.10919702798128128\n",
            "tensor(0.1092, grad_fn=<MseLossBackward0>)\n",
            "epoch 946, loss 0.10918306559324265\n",
            "tensor(0.1092, grad_fn=<MseLossBackward0>)\n",
            "epoch 947, loss 0.10916919261217117\n",
            "tensor(0.1092, grad_fn=<MseLossBackward0>)\n",
            "epoch 948, loss 0.10915540903806686\n",
            "tensor(0.1091, grad_fn=<MseLossBackward0>)\n",
            "epoch 949, loss 0.10914172232151031\n",
            "tensor(0.1091, grad_fn=<MseLossBackward0>)\n",
            "epoch 950, loss 0.10912814736366272\n",
            "tensor(0.1091, grad_fn=<MseLossBackward0>)\n",
            "epoch 951, loss 0.1091146469116211\n",
            "tensor(0.1091, grad_fn=<MseLossBackward0>)\n",
            "epoch 952, loss 0.10910125076770782\n",
            "tensor(0.1091, grad_fn=<MseLossBackward0>)\n",
            "epoch 953, loss 0.10908793658018112\n",
            "tensor(0.1091, grad_fn=<MseLossBackward0>)\n",
            "epoch 954, loss 0.10907471179962158\n",
            "tensor(0.1091, grad_fn=<MseLossBackward0>)\n",
            "epoch 955, loss 0.1090615838766098\n",
            "tensor(0.1090, grad_fn=<MseLossBackward0>)\n",
            "epoch 956, loss 0.10904855281114578\n",
            "tensor(0.1090, grad_fn=<MseLossBackward0>)\n",
            "epoch 957, loss 0.10903559625148773\n",
            "tensor(0.1090, grad_fn=<MseLossBackward0>)\n",
            "epoch 958, loss 0.10902274399995804\n",
            "tensor(0.1090, grad_fn=<MseLossBackward0>)\n",
            "epoch 959, loss 0.10900998115539551\n",
            "tensor(0.1090, grad_fn=<MseLossBackward0>)\n",
            "epoch 960, loss 0.10899730026721954\n",
            "tensor(0.1090, grad_fn=<MseLossBackward0>)\n",
            "epoch 961, loss 0.10898470878601074\n",
            "tensor(0.1090, grad_fn=<MseLossBackward0>)\n",
            "epoch 962, loss 0.10897219926118851\n",
            "tensor(0.1090, grad_fn=<MseLossBackward0>)\n",
            "epoch 963, loss 0.10895977914333344\n",
            "tensor(0.1089, grad_fn=<MseLossBackward0>)\n",
            "epoch 964, loss 0.10894742608070374\n",
            "tensor(0.1089, grad_fn=<MseLossBackward0>)\n",
            "epoch 965, loss 0.1089351698756218\n",
            "tensor(0.1089, grad_fn=<MseLossBackward0>)\n",
            "epoch 966, loss 0.10892299562692642\n",
            "tensor(0.1089, grad_fn=<MseLossBackward0>)\n",
            "epoch 967, loss 0.10891091078519821\n",
            "tensor(0.1089, grad_fn=<MseLossBackward0>)\n",
            "epoch 968, loss 0.10889893025159836\n",
            "tensor(0.1089, grad_fn=<MseLossBackward0>)\n",
            "epoch 969, loss 0.10888700932264328\n",
            "tensor(0.1089, grad_fn=<MseLossBackward0>)\n",
            "epoch 970, loss 0.10887517035007477\n",
            "tensor(0.1089, grad_fn=<MseLossBackward0>)\n",
            "epoch 971, loss 0.10886341333389282\n",
            "tensor(0.1089, grad_fn=<MseLossBackward0>)\n",
            "epoch 972, loss 0.10885174572467804\n",
            "tensor(0.1088, grad_fn=<MseLossBackward0>)\n",
            "epoch 973, loss 0.10884014517068863\n",
            "tensor(0.1088, grad_fn=<MseLossBackward0>)\n",
            "epoch 974, loss 0.10882862657308578\n",
            "tensor(0.1088, grad_fn=<MseLossBackward0>)\n",
            "epoch 975, loss 0.1088172122836113\n",
            "tensor(0.1088, grad_fn=<MseLossBackward0>)\n",
            "epoch 976, loss 0.10880585014820099\n",
            "tensor(0.1088, grad_fn=<MseLossBackward0>)\n",
            "epoch 977, loss 0.10879457741975784\n",
            "tensor(0.1088, grad_fn=<MseLossBackward0>)\n",
            "epoch 978, loss 0.10878337919712067\n",
            "tensor(0.1088, grad_fn=<MseLossBackward0>)\n",
            "epoch 979, loss 0.10877225548028946\n",
            "tensor(0.1088, grad_fn=<MseLossBackward0>)\n",
            "epoch 980, loss 0.10876121371984482\n",
            "tensor(0.1088, grad_fn=<MseLossBackward0>)\n",
            "epoch 981, loss 0.10875024646520615\n",
            "tensor(0.1087, grad_fn=<MseLossBackward0>)\n",
            "epoch 982, loss 0.10873934626579285\n",
            "tensor(0.1087, grad_fn=<MseLossBackward0>)\n",
            "epoch 983, loss 0.10872852802276611\n",
            "tensor(0.1087, grad_fn=<MseLossBackward0>)\n",
            "epoch 984, loss 0.10871778428554535\n",
            "tensor(0.1087, grad_fn=<MseLossBackward0>)\n",
            "epoch 985, loss 0.10870712995529175\n",
            "tensor(0.1087, grad_fn=<MseLossBackward0>)\n",
            "epoch 986, loss 0.10869652777910233\n",
            "tensor(0.1087, grad_fn=<MseLossBackward0>)\n",
            "epoch 987, loss 0.10868600010871887\n",
            "tensor(0.1087, grad_fn=<MseLossBackward0>)\n",
            "epoch 988, loss 0.10867554694414139\n",
            "tensor(0.1087, grad_fn=<MseLossBackward0>)\n",
            "epoch 989, loss 0.10866516828536987\n",
            "tensor(0.1087, grad_fn=<MseLossBackward0>)\n",
            "epoch 990, loss 0.10865485668182373\n",
            "tensor(0.1086, grad_fn=<MseLossBackward0>)\n",
            "epoch 991, loss 0.10864462703466415\n",
            "tensor(0.1086, grad_fn=<MseLossBackward0>)\n",
            "epoch 992, loss 0.10863447934389114\n",
            "tensor(0.1086, grad_fn=<MseLossBackward0>)\n",
            "epoch 993, loss 0.10862437635660172\n",
            "tensor(0.1086, grad_fn=<MseLossBackward0>)\n",
            "epoch 994, loss 0.10861434042453766\n",
            "tensor(0.1086, grad_fn=<MseLossBackward0>)\n",
            "epoch 995, loss 0.10860439389944077\n",
            "tensor(0.1086, grad_fn=<MseLossBackward0>)\n",
            "epoch 996, loss 0.10859449952840805\n",
            "tensor(0.1086, grad_fn=<MseLossBackward0>)\n",
            "epoch 997, loss 0.1085846945643425\n",
            "tensor(0.1086, grad_fn=<MseLossBackward0>)\n",
            "epoch 998, loss 0.10857491195201874\n",
            "tensor(0.1086, grad_fn=<MseLossBackward0>)\n",
            "epoch 999, loss 0.10856524109840393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_values_test = np.array(x_test, dtype=np.float32)\n",
        "y_values_test = np.array(y_test, dtype=np.float32)\n",
        "with torch.no_grad(): # we don't need gradients in the testing phase\n",
        "        y_pred3 = model(Variable(torch.from_numpy(x_values_test))).data.numpy()\n",
        "print(y_pred3)\n",
        "\n",
        "# The mean squared error\n",
        "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred3))\n",
        "# The coefficient of determination: 1 is perfect prediction\n",
        "print(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4Fdg9szCiKy",
        "outputId": "87ffb4e5-ca21-4bee-9764-ce0495919d10"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2.3790553]\n",
            " [2.3650782]\n",
            " [2.3580897]\n",
            " [2.3301356]\n",
            " [2.2812157]\n",
            " [2.2253075]\n",
            " [2.1903648]\n",
            " [2.1693993]\n",
            " [2.113491 ]\n",
            " [2.0785484]\n",
            " [2.07156  ]\n",
            " [2.0366173]\n",
            " [2.0016744]\n",
            " [1.9597433]\n",
            " [1.9527547]\n",
            " [1.903835 ]\n",
            " [1.8549154]\n",
            " [1.7570759]]\n",
            "Mean squared error: 0.14\n",
            "Coefficient of determination: 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# find slope using scipy least square fitting\n",
        "\n",
        "#print(x_test)\n",
        "#print(y_pred3)\n",
        "\n",
        "reg3 = stats.linregress(x_test.flatten(), y_pred3.flatten())\n",
        "print(\"Coefficients: \\n\", reg3.slope)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqwIEKbWFY_l",
        "outputId": "7767d635-abbc-478f-afb5-478e21a4e56b"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: \n",
            " -0.06988532082606426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(y_pred)\n",
        "#print(y_pred2)\n",
        "#print(y_pred3)"
      ],
      "metadata": {
        "id": "kW2vmFAkK94I"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scikit learn\n",
        "predictions = plt.scatter(y_pred, x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "F1xQStnbLc6g",
        "outputId": "ca642a22-8559-4175-f0e3-2f7e2f365929"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scipy\n",
        "predictions2 = plt.scatter(y_pred2, x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "8S92dAaEMBQ3",
        "outputId": "1ec01c02-a418-48e8-aa5c-c36bfd5c63a9"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "predictions3 = plt.scatter(y_pred3, x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "uJ675o2EKxBk",
        "outputId": "88c27b3e-ccf3-42db-bea8-9574420854be"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| *               | Coefficient |  mean sq error |\n",
        "| -----------     | ----------- |----------------|\n",
        "|  **scikitlearn**| -0.0828916  | 0.15           |\n",
        "|  **scipy**      | -0.0828916  | 0.15           |\n",
        "| **pytorch**     | -0.0828867  | 0.15           |"
      ],
      "metadata": {
        "id": "zHrjDvAJCeIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After 10000 epochs pytorch reaches the same results as with scikit learn and scipy. At 100 epochs with the current settings pytorch was not accurate at all. While it is more computationally intensive, pytorch can make better predictions even for non linear data, and so finding the slope in this case is not necessary.\n",
        "\n",
        "To build a better regression model, we can consider more independent variables and treat it as a multivariable regression model, with more coeeficients:\n",
        "\n",
        "Instead of y = mx + c\n",
        "f(x1,x2,x3) = w1.x1 + w2.x2 + w3.x3 + ... "
      ],
      "metadata": {
        "id": "xYsGNt4rDxuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qOlFPltJPV7A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}